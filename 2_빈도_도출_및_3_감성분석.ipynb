{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SangGyunYou/Github_test/blob/main/2_%EB%B9%88%EB%8F%84_%EB%8F%84%EC%B6%9C_%EB%B0%8F_3_%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH8bP7pUyZi7"
      },
      "source": [
        "# 2. 빈도 도출+그래프/워드클라우드 #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RHQ22A2mV_yi"
      },
      "outputs": [],
      "source": [
        "#### 어떤 형태소 분석을 사용할 것인가\n",
        "# 아무 기사나 가져와서 형태소(명사) 돌려봤습니다.\n",
        "# 교수님도 한국어 분석하실 때 길이 2개 이상의 명사만 사용하셨습니다.\n",
        "text = \" 더불어민주당 이재명 대선 후보 선거대책위원회 인사들이 상대 후보 지지층을 저학력 빈곤 고령층 이라고 표현하거나 여성 영입 인사 외모를 비교하는 듯한 발언을 하면서 민주당 안에서도 송영길 당대표가 문제를 지적하는 등 우려 목소리가 커지고 있다 최배근 페이스북 민주당 선대위 기본사회위원회 공동위원장인 최배근 건국대 교수는 지난 일 페이스북에 조동연 민주당 신임 공동 상임선대위원장과 이수정 국민의힘 신임 공동선대위원장의 얼굴 사진을 나란히 올리고 차이는 이라고 썼다가 논란이 됐다 별다른 설명 없이 두 사람 사진을 올리고 차이가 뭐냐고 물은 건 외모 비교라는 비판에 휩싸인 것이다 그러자 최 교수는 일부에서 외모 비교를 한다며 오버하는데 외모 비교할 거면 연예인 사진을 올렸을 것 이라며 내 눈에는 후보들의 지향 가치의 차이가 보였다 고 했다 그러나 두 사람 얼굴 사진만 보고 어떻게 지향 가치를 비교할 수 있느냐는 지적이 나온다 더불어민주당 황운하 의원이 일 올린 페이스북 글 현재 붉은 상자 부분은 수정된 상태다 페이스북 앞서 선대위 현안대응 부단장인 황운하 의원은 지난 일 페이스북에 국민의힘 윤석열 후보 지지자들을 가리켜 대부분 저학력 빈곤층 그리고 고령층 이라고 써 논란이 일었다 이에 대해 송영길 대표는 일 페이스북에 윤 후보 지지도가 높은 것은 우리 민주당이 반성해야 할 대목 이라는 제목으로 글을 올렸다 송 대표는 윤 후보를 지지하는 국민들을 비판하고 훈계하려는 자세는 매우 오만하고 위험한 태도 라고 했다 이재명 후보 수행실장인 한준호 의원은 지난 일 페이스북에 이 후보 아내 김혜경씨와 윤 후보 아내 김건희씨를 두 아이의 엄마 김혜경 토리 엄마 김건희 라고 비교해 논란이 일었다 출산 여부로 여성에게 우열을 매길 수 있다는 성차별적 인식을 보여줬다 정의당 장혜영 의원 는 것이다 민주당에서는 선대위 인사들의 차별적 언사 때문에 진보 진영이 표방했던 정치적 올바름 원칙이나 도덕적 가치가 훼손될 수 있다는 지적이 나온다 민주당 관계자는 이재명 후보가 차별금지법 제정 검토를 지시한 상황에서 이런 논란이 계속되면 유권자들에게 민주당은 위선적으로 보일 수밖에 없다 고 했다 \"\n",
        "# KoNLPY\n",
        "!pip install konlpy\n",
        "from konlpy.tag import Okt\n",
        "okt=Okt()\n",
        "\n",
        "# Komoran(코모란)\n",
        "from konlpy.tag import Komoran\n",
        "komoran=Komoran()\n",
        "\n",
        "# Hannanum(한나눔)\n",
        "from konlpy.tag import Hannanum\n",
        "hannanum=Hannanum()\n",
        "\n",
        "# Kkma (꼬꼬마)\n",
        "from konlpy.tag import Kkma\n",
        "kkma=Kkma()\n",
        "\n",
        "# 형태소분석기함수.morphs(text) 형태소 추출\n",
        "# 형태소분석기함수.pos(text) 품사 태깅\n",
        "# 형태소분석기함수.nouns(text) 명사 추출\n",
        "\n",
        "print(okt.nouns(text), komoran.nouns(text), hannanum.nouns(text), kkma.nouns(text), sep='\\n')\n",
        "# KoNLPY를 써서 쭉 해봤는데, 뭔가 이상하더라구요 '대위'라는 글자의 빈도가 되게 많아서 둘 다 대위 전역인가?? 했는데 알고보니\n",
        "# 선대위에서 선을 날리고 대위만 쓴거더라구요. '~에 대해'라고 쓸때의 '대해'도 KoNLPY에만 명사로 포함되어있습니다. '지난'도 그렇고요.\n",
        "# 그리고 아쉽게도 대부분의 형태소 분석기가 '국민의힘'과 '국민의 힘'을 국민과 힘으로 나눠서 리턴하더라구요... 유일하게 한나눔만 붙어있는 '국민의힘'을 '국민의힘'으로 리턴했습니다\n",
        "# 대부분의 분석기가 유사한 결과값을 도출하기도 해서 한나눔으로 코딩을 작성했습니다.\n",
        "\n",
        "#------------------------------------------------------\n",
        "\n",
        "#### 감성사전은 교수님께서 강의시간에 말씀해주신, NRC에서 만든 한국어 버전의 감성사전을 사용하는 게 좋을 것 같습니다.\n",
        "# NRC는 기본적으로 단어가 10 가지 감정 중 어디에 해당하는 지 알려줍니다. 그리고 10개가 대략 5개씩 positive와 negative로 나눌 수 있더라구요.\n",
        "# fear, anger, negative, sadness, disgust는 negative로 묶어서 -1점,\n",
        "# anticipation, trust, positive, joy는 positive로 묶어서 +1점\n",
        "# 예를 들어서 '격노'라는 단어는 anger emotion으로 분류되어 있으니까 -1점을 부여하는 거죠!\n",
        "# 단, 사전에는 한 단어가 두 번 이상나오는 경우('격노'도 두 번 나타납니다)도 있으니까 중복되는 애들은 제거해줘서 사용하면 될 것 같아요\n",
        "# surprise는 설렘, 놀람, 떨리는 이라는 의미와 폭발, 경보, 충격 등의 의미가 있어서 빼는 게 맞을 것 같아요\n",
        "\n",
        "print(okt.morphs(text), komoran.morphs(text), hannanum.morphs(text), kkma.morphs(text), sep='\\n')\n",
        "\n",
        "print(hannanum.pos(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0au-J29wzrf"
      },
      "source": [
        "ㄱ. 조선일보"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVh4qd3Llxo_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd132b04-f59d-4ace-bc11-2c37bedd4367"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'심상정'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "## 전처리 및 형태소(명사) 분석\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "# 뒤쪽에 빈도 그래프 도출할건데, 한글 깨지는 거 방지하기 위해 실행하는 겁니다.\n",
        "# 이거 실행하고 완료되면, 메뉴에 '런타임 - 런타임 다시 시작' 한 다음에 아래 코드부터 실행하시면 돼요\n",
        "\n",
        "!pip install konlpy\n",
        "from konlpy.tag import Hannanum\n",
        "hannanum=Hannanum()\n",
        "import re\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "cho_lee = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/chosun_lee_df.csv'\n",
        "cho_lee = pd.read_csv(cho_lee, sep=',')\n",
        "cho_lee['article'] = cho_lee[\"title\"] + \" \" + cho_lee[\"detail\"]\n",
        "\n",
        "cho_yun = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/chosun_yun_df.csv'\n",
        "cho_yun = pd.read_csv(cho_yun, sep=',')\n",
        "cho_yun['article'] = cho_yun[\"title\"] + \" \" + cho_yun[\"detail\"]\n",
        "\n",
        "# null 값 확인, 둘 다 없음\n",
        "cho_lee['article'].isnull().sum()\n",
        "cho_yun['article'].isnull().sum()\n",
        "\n",
        "# 한글 이외의 문자 모두 제거\n",
        "cho_lee['article'] = cho_lee['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "cho_yun['article'] = cho_yun['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "\n",
        "# 형태소 명사만 추출 후 2글자 이상인 단어만 뽑기\n",
        "cho_lee_noun = []\n",
        "for text in cho_lee['article']:\n",
        "  cho_lee_noun.append(hannanum.nouns(text))\n",
        "\n",
        "cho_lee_noun2 = []\n",
        "for nouns in cho_lee_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  cho_lee_noun2.append(items)\n",
        "\n",
        "cho_yun_noun = []\n",
        "for text in cho_yun['article']:\n",
        "  cho_yun_noun.append(hannanum.nouns(text))\n",
        "\n",
        "cho_yun_noun2 = []\n",
        "for nouns in cho_yun_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  cho_yun_noun2.append(items)\n",
        "\n",
        "#----------------------------------------------------------\n",
        "\n",
        "## 각 대선후보에게 가장 많이 사용한 20개의 단어\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "import seaborn as sns\n",
        "\n",
        "# 이재명 최빈출 20단어 및 그래프\n",
        "all_noun_lee = sum(cho_lee_noun2, [])\n",
        "most_20_lee = Counter(all_noun_lee).most_common(20)\n",
        "most_20_lee\n",
        "\n",
        "x, y = [], []\n",
        "for word, count in most_20_lee:\n",
        "  x.append(word)\n",
        "  y.append(count)\n",
        "plt.figure(figsize=(10, 10))\n",
        "ax = sns.barplot(x=y, y=x)\n",
        "ax.set(xlabel='Frequency', ylabel='Word')\n",
        "\n",
        "# 윤석열 최빈출 20단어 및 그래프\n",
        "all_noun_yun = sum(cho_yun_noun2, [])\n",
        "most_20_yun = Counter(all_noun_yun).most_common(20)\n",
        "most_20_yun\n",
        "\n",
        "x, y = [], []\n",
        "for word, count in most_20_yun:\n",
        "  x.append(word)\n",
        "  y.append(count)\n",
        "plt.figure(figsize=(10, 10))\n",
        "ax = sns.barplot(x=y, y=x)\n",
        "ax.set(xlabel='Frequency', ylabel='Word')\n",
        "\n",
        "# 1) '선대위'가 이재명 후보는 259번, 윤석열 후보는 '대표'와 '선대위'가 동일하게 368번, 위원장이 220번 나타났다.\n",
        "# 윤석열 후보의 선거 관련 단어 빈도가 높게 나타난 것은 선거대책위원회 구성 과정에서 이준전 국민의 힘 대표와 갈등을 빚었고,\n",
        "# 김종인 전 비상대책위원장을 선대위원장으로 데려오는 과정에 잡음이 많았기에 사용 빈도가 증가한 것이라고 생각합니다.\n",
        "# 이재명 후보의 '선대위' 단어가 259번 나온 것은, 윤석열 후보와의 비교 과정에서 나온 것으로 추론됩니다.\n",
        "# 이재명 후보도 선대위 관련 문제가 없었던 것은 아닙니다. \n",
        "# 11월 29일 조동연 서경대 교수를 공동상임선대위원장으로 임명하였으나 논란이 발생하여 12월 3일 사퇴한 바가 있습니다.\n",
        "# 다만, 본 연구의 조사 기간이 12월 5일까지였기 때문에 조동연 교수 관련 논란이 259번의 선대위 언급으로 이어졌다고 간주하는 것은 무리가 있음\n",
        "# 따라서 윤석열 후보와의 비교로 인한 빈도 증가로 판단하는 것이 적절해 보임\n",
        "# (참고로 윤석열 후보가 지방에서 잠행하던 이준석 대표와 화해하고 김종인 위원장을 선대위원장으로 데려온 날이 12월 6일입니다. 우리의 분석 기한은 5일까지였꾸요.)\n",
        "# 조사 기간 동안 윤석열 기사가 240개, 이재명 기사가 292개였으니, 수치상으로 윤석열 후보는 거의 모든 기사에서 한 번 이상 선대위 관련 이야기가 다뤄졌다고 간주할 수 있습니다.\n",
        "# 2) 이재명 단어에는 수사(444), 대장동(371), 검찰(310), 의혹(271), 억원(237)의 비리 의혹 관련 단어가 많이 나타난 반면\n",
        "# 윤석열 단어에는 수사(405), 의혹(254)가 전부라는 차이점이 있습죠.\n",
        "\n",
        "#----------------------------------------------------------\n",
        "\n",
        "## 각 대선후보의 워드클라우드(다운로드 받아서 폴더에 넣어놨습니다)\n",
        "# 회장님 말씀대로 지나치게 반복되어 단어의 빈도 파악에 방해가되는 '후보', '대선', '민주당, '더불어민주당', '국민의힘'은 제외했습니다.\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "font_path = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/NanumBarunGothic.ttf'\n",
        "# 이 글꼴 파일은 공용 폴더에 올려놨습니다\n",
        "\n",
        "def plot_cloud(wordcloud):\n",
        "  plt.figure(figsize=(40, 30))\n",
        "  plt.imshow(wordcloud)\n",
        "  plt.axis('off')\n",
        "\n",
        "# 이재명 워드클라우드\n",
        "all_noun_lee_cloud = ' '.join(all_noun_lee)\n",
        "all_noun_lee_cloud = re.sub(r'후보', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'대선', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'민주당', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'더불어민주당', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'국민의힘', ' ', all_noun_lee_cloud)\n",
        "wordcloud_coinzip = WordCloud(width=3000, height=2000, font_path=font_path, random_state=1, background_color='salmon', colormap='Pastel1', collocations=False).generate(all_noun_lee_cloud)\n",
        "plot_cloud(wordcloud_coinzip) # 이미지는 크기가 커서 새 창에서 열기하니까 랙 걸립니다. 보시려면 다운받아서 보시는 게 좋은 것 같아요\n",
        "\n",
        "# 윤석열 워드클라우드\n",
        "all_noun_yun_cloud = ' '.join(all_noun_yun)\n",
        "all_noun_yun_cloud = re.sub(r'후보', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'대선', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'민주당', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'더불어민주당', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'국민의힘', ' ', all_noun_yun_cloud)\n",
        "wordcloud_coinzip = WordCloud(width=3000, height=2000, font_path=font_path, random_state=1, background_color='salmon', colormap='Pastel1', collocations=False).generate(all_noun_yun_cloud)\n",
        "plot_cloud(wordcloud_coinzip) # 이미지는 크기가 커서 새 창에서 열기하니까 랙 걸립니다. 보시려면 다운받아서 보시는 게 좋은 것 같아요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc8fwwZuREbT"
      },
      "source": [
        "ㄴ. 중앙일보"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQcO-5TwROOK"
      },
      "outputs": [],
      "source": [
        "## 전처리 및 형태소(명사) 분석\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "# 뒤쪽에 빈도 그래프 도출할건데, 한글 깨지는 거 방지하기 위해 실행하는 겁니다.\n",
        "# 이거 실행하고 완료되면, 메뉴에 '런타임 - 런타임 다시 시작' 한 다음에 아래 코드부터 실행하시면 돼요\n",
        "\n",
        "from konlpy.tag import Hannanum\n",
        "hannanum=Hannanum()\n",
        "import re\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "joong_lee = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/joongang_lee_df.csv'\n",
        "joong_lee = pd.read_csv(joong_lee, sep=',')\n",
        "joong_lee['article'] = joong_lee[\"title\"] + \" \" + joong_lee[\"detail\"]\n",
        "\n",
        "joong_yun = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/joongang_yun_df.csv'\n",
        "joong_yun = pd.read_csv(joong_yun, sep=',')\n",
        "joong_yun['article'] = joong_yun[\"title\"] + \" \" + joong_yun[\"detail\"]\n",
        "\n",
        "# null 값 확인, 둘 다 없음\n",
        "joong_lee['article'].isnull().sum()\n",
        "joong_yun['article'].isnull().sum()\n",
        "\n",
        "# 한글 이외의 문자 모두 제거\n",
        "joong_lee['article'] = joong_lee['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "joong_yun['article'] = joong_yun['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "\n",
        "# 형태소 명사만 추출 후 2글자 이상인 단어만 뽑기\n",
        "joong_lee_noun = []\n",
        "for text in joong_lee['article']:\n",
        "  joong_lee_noun.append(hannanum.nouns(text))\n",
        "\n",
        "joong_lee_noun2 = []\n",
        "for nouns in joong_lee_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  joong_lee_noun2.append(items)\n",
        "\n",
        "joong_yun_noun = []\n",
        "for text in joong_yun['article']:\n",
        "  joong_yun_noun.append(hannanum.nouns(text))\n",
        "\n",
        "joong_yun_noun2 = []\n",
        "for nouns in joong_yun_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  joong_yun_noun2.append(items)\n",
        "\n",
        "#----------------------------------------------------------\n",
        "\n",
        "## 각 대선후보에게 가장 많이 사용한 20개의 단어\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "import seaborn as sns\n",
        "\n",
        "# 이재명 최빈출 20단어 및 그래프\n",
        "all_noun_lee = sum(joong_lee_noun2, [])\n",
        "most_20_lee = Counter(all_noun_lee).most_common(20)\n",
        "most_20_lee\n",
        "\n",
        "x, y = [], []\n",
        "for word, count in most_20_lee:\n",
        "  x.append(word)\n",
        "  y.append(count)\n",
        "plt.figure(figsize=(10, 10))\n",
        "ax = sns.barplot(x=y, y=x)\n",
        "ax.set(xlabel='Frequency', ylabel='Word')\n",
        "\n",
        "# 윤석열 최빈출 20단어 및 그래프\n",
        "all_noun_yun = sum(joong_yun_noun2, [])\n",
        "most_20_yun = Counter(all_noun_yun).most_common(20)\n",
        "most_20_yun\n",
        "\n",
        "x, y = [], []\n",
        "for word, count in most_20_yun:\n",
        "  x.append(word)\n",
        "  y.append(count)\n",
        "plt.figure(figsize=(10, 10))\n",
        "ax = sns.barplot(x=y, y=x)\n",
        "ax.set(xlabel='Frequency', ylabel='Word')\n",
        "\n",
        "# 1) 이재명 후보의 비리 관련 단어 수사(219) 특검(161) 의혹(133)과\n",
        "# 윤석열 후보의 비리 관련 단어가 수사(201) 특검(160) 의혹(144)가 동일하고, 그 수도 유사합니다.\n",
        "# 중앙일보가 이재명 후보와 윤석열 후보에 대해 각각 148개, 147개의 기사를 보도했다는 점을 미루어보면,\n",
        "# 중앙일보는 조사 기간 동안 비리 관련 기사를 다룰 때 두 후보를 비교하는 형식을 주로 취했다고 할 수 있습니다.\n",
        "# 2) 이재명 후보는 대표가 149번, 윤석열 후보는 대표 260번, 위원장이 204번 등장했는데\n",
        "# 이는 앞서와 같이 이준석 국민의 힘 대표와 김종인 전 비상대책위원장 갈등 관련 기사가 더 빈번했기 때문이라고 할 수 있다.\n",
        "\n",
        "#----------------------------------------------------------\n",
        "\n",
        "## 각 대선후보의 워드클라우드(다운로드 받아서 폴더에 넣어놨습니다)\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "font_path = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/NanumBarunGothic.ttf'\n",
        "# 이 글꼴 파일은 공용 폴더에 올려놨습니다\n",
        "\n",
        "def plot_cloud(wordcloud):\n",
        "  plt.figure(figsize=(40, 30))\n",
        "  plt.imshow(wordcloud)\n",
        "  plt.axis('off')\n",
        "\n",
        "# 이재명 워드클라우드\n",
        "all_noun_lee_cloud = ' '.join(all_noun_lee)\n",
        "all_noun_lee_cloud = re.sub(r'후보', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'대선', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'민주당', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'더불어민주당', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'국민의힘', ' ', all_noun_lee_cloud)\n",
        "wordcloud_coinzip = WordCloud(width=3000, height=2000, font_path=font_path, random_state=1, background_color='salmon', colormap='Pastel1', collocations=False).generate(all_noun_lee_cloud)\n",
        "plot_cloud(wordcloud_coinzip) # 이미지는 크기가 커서 새 창에서 열기하니까 랙 걸립니다. 보시려면 다운받아서 보시는 게 좋은 것 같아요\n",
        "\n",
        "# 윤석열 워드클라우드\n",
        "all_noun_yun_cloud = ' '.join(all_noun_yun)\n",
        "all_noun_yun_cloud = re.sub(r'후보', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'대선', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'민주당', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'더불어민주당', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'국민의힘', ' ', all_noun_yun_cloud)\n",
        "wordcloud_coinzip = WordCloud(width=3000, height=2000, font_path=font_path, random_state=1, background_color='salmon', colormap='Pastel1', collocations=False).generate(all_noun_yun_cloud)\n",
        "plot_cloud(wordcloud_coinzip) # 이미지는 크기가 커서 새 창에서 열기하니까 랙 걸립니다. 보시려면 다운받아서 보시는 게 좋은 것 같아요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxfzDju1RNtB"
      },
      "source": [
        "3. 동아일보"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PNwVH6LhRIx"
      },
      "outputs": [],
      "source": [
        "## 전처리 및 형태소(명사) 분석\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "# 뒤쪽에 빈도 그래프 도출할건데, 한글 깨지는 거 방지하기 위해 실행하는 겁니다.\n",
        "# 이거 실행하고 완료되면, 메뉴에 '런타임 - 런타임 다시 시작' 한 다음에 아래 코드부터 실행하시면 돼요\n",
        "\n",
        "from konlpy.tag import Hannanum\n",
        "hannanum=Hannanum()\n",
        "import re\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "donga_lee = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/donga_lee_df.csv'\n",
        "donga_lee = pd.read_csv(donga_lee, sep=',')\n",
        "donga_lee['article'] = donga_lee[\"title\"] + \" \" + donga_lee[\"detail\"]\n",
        "\n",
        "donga_yun = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/donga_yun_df.csv'\n",
        "donga_yun = pd.read_csv(donga_yun, sep=',')\n",
        "donga_yun['article'] = donga_yun[\"title\"] + \" \" + donga_yun[\"detail\"]\n",
        "\n",
        "# null 값 확인, 둘 다 없음\n",
        "donga_lee['article'].isnull().sum()\n",
        "donga_yun['article'].isnull().sum()\n",
        "\n",
        "# 한글 이외의 문자 모두 제거\n",
        "donga_lee['article'] = donga_lee['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "donga_yun['article'] = donga_yun['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "\n",
        "# 형태소 명사만 추출 후 2글자 이상인 단어만 뽑기\n",
        "donga_lee_noun = []\n",
        "for text in donga_lee['article']:\n",
        "  donga_lee_noun.append(hannanum.nouns(text))\n",
        "\n",
        "donga_lee_noun2 = []\n",
        "for nouns in donga_lee_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  donga_lee_noun2.append(items)\n",
        "\n",
        "donga_yun_noun = []\n",
        "for text in donga_yun['article']:\n",
        "  donga_yun_noun.append(hannanum.nouns(text))\n",
        "\n",
        "donga_yun_noun2 = []\n",
        "for nouns in donga_yun_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  donga_yun_noun2.append(items)\n",
        "\n",
        "#----------------------------------------------------------\n",
        "\n",
        "## 각 대선후보에게 가장 많이 사용한 20개의 단어\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "import seaborn as sns\n",
        "\n",
        "# 이재명 최빈출 20단어 및 그래프\n",
        "all_noun_lee = sum(donga_lee_noun2, [])\n",
        "most_20_lee = Counter(all_noun_lee).most_common(20)\n",
        "most_20_lee\n",
        "\n",
        "x, y = [], []\n",
        "for word, count in most_20_lee:\n",
        "  x.append(word)\n",
        "  y.append(count)\n",
        "plt.figure(figsize=(10, 10))\n",
        "ax = sns.barplot(x=y, y=x)\n",
        "ax.set(xlabel='Frequency', ylabel='Word')\n",
        "\n",
        "# 윤석열 최빈출 20단어 및 그래프\n",
        "all_noun_yun = sum(donga_yun_noun2, [])\n",
        "most_20_yun = Counter(all_noun_yun).most_common(20)\n",
        "most_20_yun\n",
        "\n",
        "x, y = [], []\n",
        "for word, count in most_20_yun:\n",
        "  x.append(word)\n",
        "  y.append(count)\n",
        "plt.figure(figsize=(10, 10))\n",
        "ax = sns.barplot(x=y, y=x)\n",
        "ax.set(xlabel='Frequency', ylabel='Word')\n",
        "\n",
        "# 동아일보에서는 단어 빈도의 차이가 딱히 유의미하지 않는 것 같습니다.\n",
        "# 1) 동아일보는 두 후보 모두 비리 관련 단어의 수치가 유사합니다.\n",
        "# 윤석열은 의혹(275), 수사(255), 특검(194) 이재명은 대장동(313), 수사(257), 의혹(247), 특검(231)\n",
        "# 이재명의 수치가 조금 더 높은 듯 하지만, 이는 기사 개수가 각각 이재명 240개, 윤석열 212개으로\n",
        "# 이재명의 기사 수가 조금 더 많아서 그런 것 같다.\n",
        "# 2) 동아일보는 비리 관련 기사보다는 윤석열 후보의 선대위 관련 기사를 더 많이 보도한 것으로 판단됨\n",
        "# 이재명 후보의 선대위 단어가 306회인 것에 비해 윤석열 후보의 선대위와 대표 빈도수가 각각 413과 403이기 때문\n",
        "\n",
        "#----------------------------------------------------------\n",
        "\n",
        "## 각 대선후보의 워드클라우드(다운로드 받아서 폴더에 넣어놨습니다)\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "font_path = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/NanumBarunGothic.ttf'\n",
        "# 이 글꼴 파일은 공용 폴더에 올려놨습니다\n",
        "\n",
        "def plot_cloud(wordcloud):\n",
        "  plt.figure(figsize=(40, 30))\n",
        "  plt.imshow(wordcloud)\n",
        "  plt.axis('off')\n",
        "\n",
        "# 이재명 워드클라우드\n",
        "all_noun_lee_cloud = ' '.join(all_noun_lee)\n",
        "all_noun_lee_cloud = re.sub(r'후보', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'대선', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'민주당', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'더불어민주당', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'국민의힘', ' ', all_noun_lee_cloud)\n",
        "wordcloud_coinzip = WordCloud(width=3000, height=2000, font_path=font_path, random_state=1, background_color='salmon', colormap='Pastel1', collocations=False).generate(all_noun_lee_cloud)\n",
        "plot_cloud(wordcloud_coinzip) # 이미지는 크기가 커서 새 창에서 열기하니까 랙 걸립니다. 보시려면 다운받아서 보시는 게 좋은 것 같아요\n",
        "\n",
        "# 윤석열 워드클라우드\n",
        "all_noun_yun_cloud = ' '.join(all_noun_yun)\n",
        "all_noun_yun_cloud = re.sub(r'후보', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'대선', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'민주당', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'더불어민주당', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'국민의힘', ' ', all_noun_yun_cloud)\n",
        "wordcloud_coinzip = WordCloud(width=3000, height=2000, font_path=font_path, random_state=1, background_color='salmon', colormap='Pastel1', collocations=False).generate(all_noun_yun_cloud)\n",
        "plot_cloud(wordcloud_coinzip) # 이미지는 크기가 커서 새 창에서 열기하니까 랙 걸립니다. 보시려면 다운받아서 보시는 게 좋은 것 같아요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFr3z4iAhQo7"
      },
      "source": [
        "4. 경향신문"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKZ12bewlrYt"
      },
      "outputs": [],
      "source": [
        "## 전처리 및 형태소(명사) 분석\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "# 뒤쪽에 빈도 그래프 도출할건데, 한글 깨지는 거 방지하기 위해 실행하는 겁니다.\n",
        "# 이거 실행하고 완료되면, 메뉴에 '런타임 - 런타임 다시 시작' 한 다음에 아래 코드부터 실행하시면 돼요\n",
        "\n",
        "from konlpy.tag import Hannanum\n",
        "hannanum=Hannanum()\n",
        "import re\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "kyung_lee = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/kyung_lee_df.csv'\n",
        "kyung_lee = pd.read_table(kyung_lee, sep=',')\n",
        "kyung_lee['article'] = kyung_lee[\"title\"] + \" \" + kyung_lee[\"detail\"]\n",
        "\n",
        "kyung_yun = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/kyung_yun_df.csv'\n",
        "kyung_yun = pd.read_csv(kyung_yun, sep=',')\n",
        "kyung_yun['article'] = kyung_yun[\"title\"] + \" \" + kyung_yun[\"detail\"]\n",
        "\n",
        "# null 값 확인, 둘 다 없음\n",
        "kyung_lee['article'].isnull().sum()\n",
        "kyung_yun['article'].isnull().sum()\n",
        "\n",
        "# 한글 이외의 문자 모두 제거\n",
        "kyung_lee['article'] = kyung_lee['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "kyung_yun['article'] = kyung_yun['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "\n",
        "# 형태소 명사만 추출 후 2글자 이상인 단어만 뽑기\n",
        "kyung_lee_noun = []\n",
        "for text in kyung_lee['article']:\n",
        "  kyung_lee_noun.append(hannanum.nouns(text))\n",
        "\n",
        "kyung_lee_noun2 = []\n",
        "for nouns in kyung_lee_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  kyung_lee_noun2.append(items)\n",
        "\n",
        "kyung_yun_noun = []\n",
        "for text in kyung_yun['article']:\n",
        "  kyung_yun_noun.append(hannanum.nouns(text))\n",
        "\n",
        "kyung_yun_noun2 = []\n",
        "for nouns in kyung_yun_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  kyung_yun_noun2.append(items)\n",
        "\n",
        "#----------------------------------------------------------\n",
        "\n",
        "## 각 대선후보에게 가장 많이 사용한 20개의 단어\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "import seaborn as sns\n",
        "\n",
        "# 이재명 최빈출 20단어 및 그래프\n",
        "all_noun_lee = sum(kyung_lee_noun2, [])\n",
        "most_20_lee = Counter(all_noun_lee).most_common(20)\n",
        "most_20_lee\n",
        "\n",
        "x, y = [], []\n",
        "for word, count in most_20_lee:\n",
        "  x.append(word)\n",
        "  y.append(count)\n",
        "plt.figure(figsize=(10, 10))\n",
        "ax = sns.barplot(x=y, y=x)\n",
        "ax.set(xlabel='Frequency', ylabel='Word')\n",
        "\n",
        "# 윤석열 최빈출 20단어 및 그래프\n",
        "all_noun_yun = sum(kyung_yun_noun2, [])\n",
        "most_20_yun = Counter(all_noun_yun).most_common(20)\n",
        "most_20_yun\n",
        "\n",
        "x, y = [], []\n",
        "for word, count in most_20_yun:\n",
        "  x.append(word)\n",
        "  y.append(count)\n",
        "plt.figure(figsize=(10, 10))\n",
        "ax = sns.barplot(x=y, y=x)\n",
        "ax.set(xlabel='Frequency', ylabel='Word')\n",
        "\n",
        "# 경향신문은 진보 성향의 신문사 답게, 윤석열 후보의 문제를 집중적으로 기사화했다.\n",
        "# 1) 이재명 후보의 단어 중 빈도가 300과 유사하거나 그 이상인 단어는 후보(1780), 대선(649), 민주당(387), 이재명(383), 국민의힘(299)정도 인것에 반해\n",
        "# 윤석열 후보는 선대위 관련 단어가 대표(417), 선대위(346), 위원장(306)로, 모두 300 이상의 빈도수만으로 나타냄.\n",
        "# 2) 또한 이재명 후보의 비리 관련 단어는 대장동(185), 의혹(184), 수사(162)인 것에 비해\n",
        "# 윤석열 후보의 비리 관련 단어는 수사(246), 의혹(243)으로 역시나 빈도가 높음.\n",
        "# 이는 경향신문의 후보별 보도 기사 수가 이재명 187개, 윤석열 214개인 것을 고려하더라도 높은 수치\n",
        "\n",
        "#----------------------------------------------------------\n",
        "\n",
        "## 각 대선후보의 워드클라우드(다운로드 받아서 폴더에 넣어놨습니다)\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "font_path = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/NanumBarunGothic.ttf'\n",
        "# 이 글꼴 파일은 공용 폴더에 올려놨습니다\n",
        "\n",
        "def plot_cloud(wordcloud):\n",
        "  plt.figure(figsize=(40, 30))\n",
        "  plt.imshow(wordcloud)\n",
        "  plt.axis('off')\n",
        "\n",
        "# 이재명 워드클라우드\n",
        "all_noun_lee_cloud = ' '.join(all_noun_lee)\n",
        "all_noun_lee_cloud = re.sub(r'후보', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'대선', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'민주당', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'더불어민주당', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'국민의힘', ' ', all_noun_lee_cloud)\n",
        "wordcloud_coinzip = WordCloud(width=3000, height=2000, font_path=font_path, random_state=1, background_color='salmon', colormap='Pastel1', collocations=False).generate(all_noun_lee_cloud)\n",
        "plot_cloud(wordcloud_coinzip) # 이미지는 크기가 커서 새 창에서 열기하니까 랙 걸립니다. 보시려면 다운받아서 보시는 게 좋은 것 같아요\n",
        "\n",
        "# 윤석열 워드클라우드\n",
        "all_noun_yun_cloud = ' '.join(all_noun_yun)\n",
        "all_noun_yun_cloud = re.sub(r'후보', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'대선', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'민주당', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'더불어민주당', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'국민의힘', ' ', all_noun_yun_cloud)\n",
        "wordcloud_coinzip = WordCloud(width=3000, height=2000, font_path=font_path, random_state=1, background_color='salmon', colormap='Pastel1', collocations=False).generate(all_noun_yun_cloud)\n",
        "plot_cloud(wordcloud_coinzip) # 이미지는 크기가 커서 새 창에서 열기하니까 랙 걸립니다. 보시려면 다운받아서 보시는 게 좋은 것 같아요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIuT_RaswSln"
      },
      "source": [
        "ㅁ. 한겨례 신문"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYo-OA5jwSZ4"
      },
      "outputs": [],
      "source": [
        "## 전처리 및 형태소(명사) 분석\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "# 뒤쪽에 빈도 그래프 도출할건데, 한글 깨지는 거 방지하기 위해 실행하는 겁니다.\n",
        "# 이거 실행하고 완료되면, 메뉴에 '런타임 - 런타임 다시 시작' 한 다음에 아래 코드부터 실행하시면 돼요\n",
        "\n",
        "from konlpy.tag import Hannanum\n",
        "hannanum=Hannanum()\n",
        "import re\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "han_lee = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/han_lee_df.csv'\n",
        "han_lee = pd.read_table(han_lee, sep=',')\n",
        "han_lee['article'] = han_lee[\"title\"] + \" \" + han_lee[\"detail\"]\n",
        "\n",
        "han_yun = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/han_yun_df.csv'\n",
        "han_yun = pd.read_csv(han_yun, sep=',')\n",
        "han_yun['article'] = han_yun[\"title\"] + \" \" + han_yun[\"detail\"]\n",
        "\n",
        "# null 값 확인, 둘 다 없음\n",
        "han_lee['article'].isnull().sum()\n",
        "han_yun['article'].isnull().sum()\n",
        "\n",
        "# 한글 이외의 문자 모두 제거\n",
        "han_lee['article'] = han_lee['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "han_yun['article'] = han_yun['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "\n",
        "# 형태소 명사만 추출 후 2글자 이상인 단어만 뽑기\n",
        "han_lee_noun = []\n",
        "for text in han_lee['article']:\n",
        "  han_lee_noun.append(hannanum.nouns(text))\n",
        "\n",
        "han_lee_noun2 = []\n",
        "for nouns in han_lee_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  han_lee_noun2.append(items)\n",
        "\n",
        "han_yun_noun = []\n",
        "for text in han_yun['article']:\n",
        "  han_yun_noun.append(hannanum.nouns(text))\n",
        "\n",
        "han_yun_noun2 = []\n",
        "for nouns in han_yun_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  han_yun_noun2.append(items)\n",
        "\n",
        "#----------------------------------------------------------\n",
        "\n",
        "## 각 대선후보에게 가장 많이 사용한 20개의 단어\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "import seaborn as sns\n",
        "\n",
        "# 이재명 최빈출 20단어 및 그래프\n",
        "all_noun_lee = sum(han_lee_noun2, [])\n",
        "most_20_lee = Counter(all_noun_lee).most_common(20)\n",
        "most_20_lee\n",
        "\n",
        "x, y = [], []\n",
        "for word, count in most_20_lee:\n",
        "  x.append(word)\n",
        "  y.append(count)\n",
        "plt.figure(figsize=(10, 10))\n",
        "ax = sns.barplot(x=y, y=x)\n",
        "ax.set(xlabel='Frequency', ylabel='Word')\n",
        "\n",
        "# 윤석열 최빈출 20단어 및 그래프\n",
        "all_noun_yun = sum(han_yun_noun2, [])\n",
        "most_20_yun = Counter(all_noun_yun).most_common(20)\n",
        "most_20_yun\n",
        "\n",
        "x, y = [], []\n",
        "for word, count in most_20_yun:\n",
        "  x.append(word)\n",
        "  y.append(count)\n",
        "plt.figure(figsize=(10, 10))\n",
        "ax = sns.barplot(x=y, y=x)\n",
        "ax.set(xlabel='Frequency', ylabel='Word')\n",
        "\n",
        "# 한겨례도 경향신문과 유사하게 이재명과 윤석열에 온도차를 보인다.\n",
        "# 1) 이재명의 선대위의 빈도가 226인 것에 비해 윤석열은 대표 292번, 선대위 278번으로 더 잦았음\n",
        "# 한겨례는 이재명 144개, 윤석열 177개의 기사를 보도했는데, 두 후보에 대해 각각 148개, 147개의 기사를 보도한 중앙일보와 비교하면\n",
        "# 한겨례의 윤석열 선대위 관련 단어의 빈도가 높은 편이다. 중앙일보는 윤석열 후보 관련하여 대표 260번, 위원장을 204번 사용함\n",
        "# 2) 다만 비리 관련 단어에서는 이재명 문제(189), 조사(135)와 윤석열의 의혹(199), 수사(190), 문제(179)으로\n",
        "# 그 차이가 크지 않음\n",
        "\n",
        "#----------------------------------------------------------\n",
        "\n",
        "## 각 대선후보의 워드클라우드(다운로드 받아서 폴더에 넣어놨습니다)\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "font_path = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/NanumBarunGothic.ttf'\n",
        "# 이 글꼴 파일은 공용 폴더에 올려놨습니다\n",
        "\n",
        "def plot_cloud(wordcloud):\n",
        "  plt.figure(figsize=(40, 30))\n",
        "  plt.imshow(wordcloud)\n",
        "  plt.axis('off')\n",
        "\n",
        "# 이재명 워드클라우드\n",
        "all_noun_lee_cloud = ' '.join(all_noun_lee)\n",
        "all_noun_lee_cloud = re.sub(r'후보', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'대선', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'민주당', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'더불어민주당', ' ', all_noun_lee_cloud)\n",
        "all_noun_lee_cloud = re.sub(r'국민의힘', ' ', all_noun_lee_cloud)\n",
        "wordcloud_coinzip = WordCloud(width=3000, height=2000, font_path=font_path, random_state=1, background_color='salmon', colormap='Pastel1', collocations=False).generate(all_noun_lee_cloud)\n",
        "plot_cloud(wordcloud_coinzip) # 이미지는 크기가 커서 새 창에서 열기하니까 랙 걸립니다. 보시려면 다운받아서 보시는 게 좋은 것 같아요\n",
        "\n",
        "# 윤석열 워드클라우드\n",
        "all_noun_yun_cloud = ' '.join(all_noun_yun)\n",
        "all_noun_yun_cloud = re.sub(r'후보', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'대선', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'민주당', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'더불어민주당', ' ', all_noun_yun_cloud)\n",
        "all_noun_yun_cloud = re.sub(r'국민의힘', ' ', all_noun_yun_cloud)\n",
        "wordcloud_coinzip = WordCloud(width=3000, height=2000, font_path=font_path, random_state=1, background_color='salmon', colormap='Pastel1', collocations=False).generate(all_noun_yun_cloud)\n",
        "plot_cloud(wordcloud_coinzip) # 이미지는 크기가 커서 새 창에서 열기하니까 랙 걸립니다. 보시려면 다운받아서 보시는 게 좋은 것 같아요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io_CrSX9yEBe"
      },
      "source": [
        "# 3. 감성분석 #"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 미리 돌릴 애들\n",
        "\n",
        "빈도를 분석하기 위한 전처리 과정에서 한나눔 형태소 분석기를 활용하여 명사만 추출하였고, 그중에서 2단어 이상의 명사만을 분석에 활용하였다.\n",
        "감성분석의 독립변수로 사용되는 단어에는 TF-IDF를 적용하여 후보, 이재명, 윤석열 등의 높은 빈도를 보이는 단어에 낮은 가중치를 부여하였다.\n",
        "종속변수는 값이 따로 존재하지 않기 때문에, 한국어 버전의 NRC 용어 사전을 이용하여 계산하였다.\n",
        "일차적으로 한국어 버전의 NRC에서 제시한 감정을 negative와 positive의 2종류로 나누어 negative에는 -1점, positive에는 +1점의 감성점수를 부여하였다.\n",
        "그 후 문장을 이루는 단어 감성점수를 더해서 총합이 양수면 종속변수에 positive를, 음수면 negative를 할당하였다.\n",
        "또한, 각 감정의 분류는 다음과 같다.\n",
        "anger, disgust, fear, sadness는 negative로, anticipation, joy, trust는 positive로 설정하였다.\n",
        "\n",
        "NRC 감정 중 surprise는 기적, 흥미진진한, 기쁨, 순진한 등의 긍정적인 의미와 폭발, 공포, 충격, 살인 등의 부정적 의미 혼재되어 있어 제외하였고,\n",
        "영단어 버전과 달리 negative와 positive 단어가 존재하지 않았다는 차이점도 있었다.\n",
        "\n",
        "제목과 본문에 대한 감성분석을 따로 진행이 불가합니다 제목에 남아있는 단어의 수가 너무 적어서 분석을 돌릴 수가 없겠더라구요"
      ],
      "metadata": {
        "id": "rsc2KAYGVGxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy\n",
        "from konlpy.tag import Hannanum\n",
        "hannanum=Hannanum()\n",
        "import re\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "cho_lee = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/chosun_lee_df.csv'\n",
        "cho_lee = pd.read_csv(cho_lee, sep=',')\n",
        "cho_lee['article'] = cho_lee[\"title\"] + \" \" + cho_lee[\"detail\"]\n",
        "\n",
        "cho_yun = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/chosun_yun_df.csv'\n",
        "cho_yun = pd.read_csv(cho_yun, sep=',')\n",
        "cho_yun['article'] = cho_yun[\"title\"] + \" \" + cho_yun[\"detail\"]\n",
        "\n",
        "cho_lee['article'] = cho_lee['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "cho_yun['article'] = cho_yun['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "\n",
        "cho_lee_noun = []\n",
        "for text in cho_lee['article']:\n",
        "  cho_lee_noun.append(hannanum.nouns(text))\n",
        "cho_lee_noun2 = []\n",
        "for nouns in cho_lee_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  cho_lee_noun2.append(items)\n",
        "\n",
        "cho_yun_noun = []\n",
        "for text in cho_yun['article']:\n",
        "  cho_yun_noun.append(hannanum.nouns(text))\n",
        "cho_yun_noun2 = []\n",
        "for nouns in cho_yun_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  cho_yun_noun2.append(items)\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "joong_lee = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/joongang_lee_df.csv'\n",
        "joong_lee = pd.read_csv(joong_lee, sep=',')\n",
        "joong_lee['article'] = joong_lee[\"title\"] + \" \" + joong_lee[\"detail\"]\n",
        "\n",
        "joong_yun = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/joongang_yun_df.csv'\n",
        "joong_yun = pd.read_csv(joong_yun, sep=',')\n",
        "joong_yun['article'] = joong_yun[\"title\"] + \" \" + joong_yun[\"detail\"]\n",
        "\n",
        "joong_lee['article'] = joong_lee['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "joong_yun['article'] = joong_yun['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "\n",
        "joong_lee_noun = []\n",
        "for text in joong_lee['article']:\n",
        "  joong_lee_noun.append(hannanum.nouns(text))\n",
        "joong_lee_noun2 = []\n",
        "for nouns in joong_lee_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  joong_lee_noun2.append(items)\n",
        "\n",
        "joong_yun_noun = []\n",
        "for text in joong_yun['article']:\n",
        "  joong_yun_noun.append(hannanum.nouns(text))\n",
        "joong_yun_noun2 = []\n",
        "for nouns in joong_yun_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  joong_yun_noun2.append(items)\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "donga_lee = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/donga_lee_df.csv'\n",
        "donga_lee = pd.read_csv(donga_lee, sep=',')\n",
        "donga_lee['article'] = donga_lee[\"title\"] + \" \" + donga_lee[\"detail\"]\n",
        "\n",
        "donga_yun = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/donga_yun_df.csv'\n",
        "donga_yun = pd.read_csv(donga_yun, sep=',')\n",
        "donga_yun['article'] = donga_yun[\"title\"] + \" \" + donga_yun[\"detail\"]\n",
        "\n",
        "donga_lee['article'] = donga_lee['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "donga_yun['article'] = donga_yun['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "\n",
        "donga_lee_noun = []\n",
        "for text in donga_lee['article']:\n",
        "  donga_lee_noun.append(hannanum.nouns(text))\n",
        "donga_lee_noun2 = []\n",
        "for nouns in donga_lee_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  donga_lee_noun2.append(items)\n",
        "\n",
        "donga_yun_noun = []\n",
        "for text in donga_yun['article']:\n",
        "  donga_yun_noun.append(hannanum.nouns(text))\n",
        "donga_yun_noun2 = []\n",
        "for nouns in donga_yun_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  donga_yun_noun2.append(items)\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "kyung_lee = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/kyung_lee_df.csv'\n",
        "kyung_lee = pd.read_table(kyung_lee, sep=',')\n",
        "kyung_lee['article'] = kyung_lee[\"title\"] + \" \" + kyung_lee[\"detail\"]\n",
        "\n",
        "kyung_yun = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/kyung_yun_df.csv'\n",
        "kyung_yun = pd.read_csv(kyung_yun, sep=',')\n",
        "kyung_yun['article'] = kyung_yun[\"title\"] + \" \" + kyung_yun[\"detail\"]\n",
        "\n",
        "kyung_lee['article'] = kyung_lee['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "kyung_yun['article'] = kyung_yun['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "\n",
        "kyung_lee_noun = []\n",
        "for text in kyung_lee['article']:\n",
        "  kyung_lee_noun.append(hannanum.nouns(text))\n",
        "kyung_lee_noun2 = []\n",
        "for nouns in kyung_lee_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  kyung_lee_noun2.append(items)\n",
        "\n",
        "kyung_yun_noun = []\n",
        "for text in kyung_yun['article']:\n",
        "  kyung_yun_noun.append(hannanum.nouns(text))\n",
        "kyung_yun_noun2 = []\n",
        "for nouns in kyung_yun_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  kyung_yun_noun2.append(items)\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "han_lee = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/han_lee_df.csv'\n",
        "han_lee = pd.read_table(han_lee, sep=',')\n",
        "han_lee['article'] = han_lee[\"title\"] + \" \" + han_lee[\"detail\"]\n",
        "\n",
        "han_yun = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Team_Text Mining/han_yun_df.csv'\n",
        "han_yun = pd.read_csv(han_yun, sep=',')\n",
        "han_yun['article'] = han_yun[\"title\"] + \" \" + han_yun[\"detail\"]\n",
        "\n",
        "han_lee['article'] = han_lee['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "han_yun['article'] = han_yun['article'].apply(lambda x: re.sub(r'[^ㄱ-ㅎ| 가-힣]+', ' ', x))\n",
        "\n",
        "han_lee_noun = []\n",
        "for text in han_lee['article']:\n",
        "  han_lee_noun.append(hannanum.nouns(text))\n",
        "han_lee_noun2 = []\n",
        "for nouns in han_lee_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  han_lee_noun2.append(items)\n",
        "\n",
        "han_yun_noun = []\n",
        "for text in han_yun['article']:\n",
        "  han_yun_noun.append(hannanum.nouns(text))\n",
        "han_yun_noun2 = []\n",
        "for nouns in han_yun_noun:\n",
        "  items = [noun for noun in nouns if len(noun) > 1]\n",
        "  han_yun_noun2.append(items)"
      ],
      "metadata": {
        "id": "m4a_k9cNVF_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxnOSGjpyqNy"
      },
      "source": [
        "NRC 사전 제작"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MQR0hG1ypUk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "NRC_path = '/content/gdrive/My Drive/Pr(광고홍보학부, AI MBA)/AI MBA/3. 가을 학기/3_2. 텍스트 마이닝★/Asg_Text Mining/Data/Korean-ko-NRC-Emotion-Intensity-Lexicon-v1.txt'\n",
        "NRC0 = pd.read_csv(NRC_path, engine='python', header=None, sep='\\t')\n",
        "NRC0 = NRC0[1:]\n",
        "NRC0 = NRC0[[1, 2]] #9921개\n",
        "\n",
        "# NRC0 제작: 한국어로 번역이 안된 NO TRANSLATION과 surprise, 중복되는 값 제거\n",
        "index1 = NRC0[NRC0[1]=='NO TRANSLATION'].index\n",
        "NRC0 = NRC0.drop(index1)\n",
        "index2 = NRC0[NRC0[2]=='surprise'].index\n",
        "NRC0 = NRC0.drop(index2)\n",
        "NRC0 = NRC0.drop_duplicates()\n",
        "NRC0 = NRC0.reset_index(drop=True)\n",
        "NRC0 # 9개의 감정으로 이뤄진 7806개의 데이터프레임\n",
        "\n",
        "# NRC1 제작: NRC0의 감정을 1과 -1로 변환\n",
        "from copy import deepcopy\n",
        "NRC1 = deepcopy(NRC0)\n",
        "conditions  = (NRC1[2]=='anticipation') | (NRC1[2]=='joy') | (NRC1[2]=='trust') | (NRC1[2]=='positive')\n",
        "NRC1[2] = np.where(conditions, 1, -1)\n",
        "NRC1 # 1과 -1로 이뤄진 7806개의 데이터프레임\n",
        "\n",
        "# NRC2 제작: NRC0과 NRC1을 합병\n",
        "NRC2 = pd.concat([NRC0, NRC1[2]], axis=1)\n",
        "NRC2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EEL50JrIeDW"
      },
      "source": [
        "## 본격적인 분석\n",
        "각 언론사별로 8개의 단어를 어떻게 사용했는지의 그래프와\n",
        "8개의 단어를 pos, neg로 분류했을 때의 그래프를 모두 도출 가능하게 코드 적어놨습니다. \n",
        "\n",
        "아래 부분에서 결론으로 적어두었는데, 8가지 감정으로 나타낸 그래프 사용하면 될 것 같습니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROtMj9bvx9oT"
      },
      "source": [
        "ㄱ. 조선일보"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkC3Ncugx9dr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "## 감정단어 빈도 그래프, 한 번에 5분 정도 소요\n",
        "# 이재명\n",
        "emotion = []\n",
        "sent_list = []\n",
        "for sentences in cho_lee_noun2:\n",
        "  match_words = [sentence for sentence in sentences if sentence in list(NRC2[1])]\n",
        "  sentiment = 0\n",
        "  for word in match_words:\n",
        "    temp = list(NRC2.iloc[np.where(NRC2[1] == word)[0],1])[0] # 감정 리턴\n",
        "    score = list(NRC2.iloc[np.where(NRC2[1] == word)[0],2])[0] # 감정에 해당하는 수치 리턴\n",
        "    emotion.append(temp)\n",
        "    sentiment += int(score)\n",
        "  sent_list.append(sentiment)\n",
        "cho_lee_sent = pd.DataFrame(sent_list, columns = ['sentiment'])\n",
        "cho_lee_sent['sentiment'] = np.where(cho_lee_sent['sentiment']>=0, 'positive', 'negative')\n",
        "\n",
        "new_emotion = []\n",
        "for emo in emotion:\n",
        "  if emo == 'anger' or emo == 'disgust' or emo == 'fear' or emo == 'sadness' or emo == 'negative':\n",
        "    emo = 'negative'\n",
        "    new_emotion.append(emo)\n",
        "  else:\n",
        "    emo = 'positive'\n",
        "    new_emotion.append(emo)\n",
        "sentiment_result1 = pd.Series(emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정 그대로를 빈도로 표현한 것\n",
        "# anger(2668), trust(2356), anticipation(2029), fear(1246), joy(1039), disgust(361), sadness(185)\n",
        "\n",
        "sentiment_result1 = pd.Series(new_emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정을 positive와 negative로 분류해 표현한 것\n",
        "# positive(5424), negative(4460)\n",
        "# 결과 이미지는 3. 감성분석 폴더에 넣어놨습니다.\n",
        "\n",
        "\n",
        "# 윤석열\n",
        "emotion = []\n",
        "sent_list = []\n",
        "for sentences in cho_yun_noun2:\n",
        "  match_words = [sentence for sentence in sentences if sentence in list(NRC2[1])] # NRC2에 포함된 단어 \n",
        "  sentiment = 0\n",
        "  for word in match_words:\n",
        "    temp = list(NRC2.iloc[np.where(NRC2[1] == word)[0],1])[0] # 감정 리턴\n",
        "    score = list(NRC2.iloc[np.where(NRC2[1] == word)[0],2])[0] # 감정에 해당하는 수치 리턴\n",
        "    emotion.append(temp)\n",
        "    sentiment += int(score)\n",
        "  sent_list.append(sentiment)\n",
        "cho_yun_sent = pd.DataFrame(sent_list, columns = ['sentiment'])\n",
        "cho_yun_sent['sentiment'] = np.where(cho_yun_sent['sentiment']>=0, 'positive', 'negative')\n",
        "\n",
        "new_emotion = []\n",
        "for emo in emotion:\n",
        "  if emo == 'anger' or emo == 'disgust' or emo == 'fear' or emo == 'sadness' or emo == 'negative':\n",
        "    emo = 'negative'\n",
        "    new_emotion.append(emo)\n",
        "  else:\n",
        "    emo = 'positive'\n",
        "    new_emotion.append(emo)\n",
        "sentiment_result1 = pd.Series(emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정 그대로를 빈도로 표현한 것\n",
        "# trust(2079), anger(2034), anticipation(1747), fear(1125), joy(867), disgust(319), sadness(141)\n",
        "\n",
        "sentiment_result1 = pd.Series(new_emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정을 positive와 negative로 분류해 표현한 것\n",
        "# positive(4693), negative(3619)\n",
        "# 결과 이미지는 3. 감성분석 폴더에 넣어놨습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpNhb_FCK2UZ"
      },
      "outputs": [],
      "source": [
        "## 감성분석\n",
        "# 감성분석의 독립변수로 사용되는 단어에는 TF-IDF를 적용함으로써 높은 빈도 단어에 낮은 가중치를, 기사별 차이를 보이는 단어에는 높은 가중치를 부여하였다.\n",
        "# 모형 제작에는 로지스틱과 랜덤포레스트를 적용하여 비교하였다.\n",
        "# 랜덤포레스트은 정확도를 확보하기 위해 cross-validation을 적용하였고, 이때의 cross-validation 횟수는 10번으로 설정하였다.\n",
        "\n",
        "# 이재명 독립변수 TF-IDF화\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf_vect = TfidfVectorizer()\n",
        "\n",
        "x_cho_lee = []\n",
        "for words in cho_lee_noun2:\n",
        "  words_join = ' '.join(words)\n",
        "  x_cho_lee.append(words_join)\n",
        "\n",
        "vect = tf_vect.fit(x_cho_lee)\n",
        "x_cho_lee = vect.transform(x_cho_lee)\n",
        "\n",
        "# training / test data 분류\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_cho_lee, cho_lee_sent, stratify=cho_lee_sent, test_size=0.2, random_state=15) \n",
        "\n",
        "# 로지스틱에 적용\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "logistic = LogisticRegression()\n",
        "logistic.fit(x_train, y_train)\n",
        "\n",
        "print(logistic.score(x_train, y_train)) # estimation = 0.7982832618025751\n",
        "print(logistic.score(x_test, y_test)) # prediction = 0.6949152542372882\n",
        "\n",
        "# 랜덤포레스트에 적용\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "forest = RandomForestClassifier(n_estimators = 100, n_jobs = -1, random_state=2018)\n",
        "forest = forest.fit(x_train, y_train)\n",
        " \n",
        "print(forest.score(x_train, y_train)) # estimation = 1.0\n",
        "print(forest.score(x_test, y_test)) # preditction = 0.6610169491525424\n",
        "\n",
        "# 랜덤포레스트 k-fold 각각의 ROC AUC 값\n",
        "k_fold = cross_val_score(forest, x_train, y_train, cv=10, scoring='roc_auc')\n",
        "print(k_fold)\n",
        "# [0.82421875 0.5859375  0.6962963  0.61666667 0.80416667 0.75416667 0.71666667 0.9        0.6        0.74583333]\n",
        "\n",
        "# k-fold 전체 ROC AUC 평균치\n",
        "score = np.mean(k_fold)\n",
        "print(score) # 0.7243952546296295\n",
        "\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "# 윤석열 독립변수 TF-IDF화\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf_vect = TfidfVectorizer()\n",
        "\n",
        "x_cho_yun = []\n",
        "for words in cho_yun_noun2:\n",
        "  words_join = ' '.join(words)\n",
        "  x_cho_yun.append(words_join)\n",
        "\n",
        "vect = tf_vect.fit(x_cho_yun)\n",
        "x_cho_yun = vect.transform(x_cho_yun)\n",
        "\n",
        "# training / test data 분류\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_cho_yun, cho_yun_sent, stratify=cho_yun_sent, test_size=0.2, random_state=15) \n",
        "\n",
        "# 로지스틱에 적용\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "logistic = LogisticRegression()\n",
        "logistic.fit(x_train, y_train)\n",
        "\n",
        "print(logistic.score(x_train, y_train)) # estimation = 0.796875\n",
        "print(logistic.score(x_test, y_test)) # prediction = 0.625\n",
        "\n",
        "# 랜덤포레스트에 적용\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "forest = RandomForestClassifier(n_estimators = 100, n_jobs = -1, random_state=2018)\n",
        "forest = forest.fit(x_train, y_train)\n",
        " \n",
        "print(forest.score(x_train, y_train)) # estimation = 1.0\n",
        "print(forest.score(x_test, y_test)) # preditction = 0.6875\n",
        "\n",
        "# 랜덤포레스트 k-fold 각각의 ROC AUC 값\n",
        "k_fold = cross_val_score(forest, x_train, y_train, cv=10, scoring='roc_auc')\n",
        "print(k_fold)\n",
        "# [0.78571429 0.74725275 0.92307692 0.87179487 0.76923077 0.78846154 0.9047619  0.74404762 0.78571429 0.69047619]\n",
        "\n",
        "# k-fold 전체 ROC AUC 평균치\n",
        "score = np.mean(k_fold)\n",
        "print(score) # 0.8010531135531135"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-TTz7FED4ET"
      },
      "source": [
        "ㄴ. 중앙일보"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxv36FA1D38K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "## 감정단어 빈도 그래프, 한 번에 5분 정도 소요\n",
        "# 이재명\n",
        "emotion = []\n",
        "sent_list = []\n",
        "for sentences in joong_lee_noun2:\n",
        "  match_words = [sentence for sentence in sentences if sentence in list(NRC2[1])] # NRC2에 포함된 단어 \n",
        "  sentiment = 0\n",
        "  for word in match_words:\n",
        "    temp = list(NRC2.iloc[np.where(NRC2[1] == word)[0],1])[0] # 감정 리턴\n",
        "    score = list(NRC2.iloc[np.where(NRC2[1] == word)[0],2])[0] # 감정에 해당하는 수치 리턴\n",
        "    emotion.append(temp)\n",
        "    sentiment += int(score)\n",
        "  sent_list.append(sentiment)\n",
        "joong_lee_sent = pd.DataFrame(sent_list, columns = ['sentiment'])\n",
        "joong_lee_sent['sentiment'] = np.where(joong_lee_sent['sentiment']>=0, 'positive', 'negative')\n",
        "\n",
        "new_emotion = []\n",
        "for emo in emotion:\n",
        "  if emo == 'anger' or emo == 'disgust' or emo == 'fear' or emo == 'sadness' or emo == 'negative':\n",
        "    emo = 'negative'\n",
        "    new_emotion.append(emo)\n",
        "  else:\n",
        "    emo = 'positive'\n",
        "    new_emotion.append(emo)\n",
        "sentiment_result1 = pd.Series(emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정 그대로를 빈도로 표현한 것\n",
        "# trust(1414), anger(1385), anticipation(1198), fear(613), joy(606), disgust(170), sadness(122)\n",
        "sentiment_result1 = pd.Series(new_emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정을 positive와 negative로 분류해 표현한 것\n",
        "# positive(3218), negative(2290)\n",
        "# 결과 이미지는 3. 감성분석 폴더에 넣어놨습니다.\n",
        "\n",
        "\n",
        "# 윤석열\n",
        "emotion = []\n",
        "sent_list = []\n",
        "for sentences in joong_yun_noun2:\n",
        "  match_words = [sentence for sentence in sentences if sentence in list(NRC2[1])] # NRC2에 포함된 단어 \n",
        "  sentiment = 0\n",
        "  for word in match_words:\n",
        "    temp = list(NRC2.iloc[np.where(NRC2[1] == word)[0],1])[0] # 감정 리턴\n",
        "    score = list(NRC2.iloc[np.where(NRC2[1] == word)[0],2])[0] # 감정에 해당하는 수치 리턴\n",
        "    emotion.append(temp)\n",
        "    sentiment += int(score)\n",
        "  sent_list.append(sentiment)\n",
        "joong_yun_sent = pd.DataFrame(sent_list, columns = ['sentiment'])\n",
        "joong_yun_sent['sentiment'] = np.where(joong_yun_sent['sentiment']>=0, 'positive', 'negative')\n",
        "\n",
        "new_emotion = []\n",
        "for emo in emotion:\n",
        "  if emo == 'anger' or emo == 'disgust' or emo == 'fear' or emo == 'sadness' or emo == 'negative':\n",
        "    emo = 'negative'\n",
        "    new_emotion.append(emo)\n",
        "  else:\n",
        "    emo = 'positive'\n",
        "    new_emotion.append(emo)\n",
        "sentiment_result1 = pd.Series(emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정 그대로를 빈도로 표현한 것\n",
        "# trust(1527), anger(1410), anticipation(1215), fear(657), joy(578), disgust(212), sadness(99)\n",
        "\n",
        "sentiment_result1 = pd.Series(new_emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정을 positive와 negative로 분류해 표현한 것\n",
        "# positive(3320), negative(2378)\n",
        "# 결과 이미지는 3. 감성분석 폴더에 넣어놨습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5pQTjzwLYI_"
      },
      "outputs": [],
      "source": [
        "## 감성분석\n",
        "# 이재명 독립변수 TF-IDF화\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf_vect = TfidfVectorizer()\n",
        "\n",
        "x_joong_lee = []\n",
        "for words in joong_lee_noun2:\n",
        "  words_join = ' '.join(words)\n",
        "  x_joong_lee.append(words_join)\n",
        "\n",
        "vect = tf_vect.fit(x_joong_lee)\n",
        "x_joong_lee = vect.transform(x_joong_lee)\n",
        "\n",
        "# training / test data 분류\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_joong_lee, joong_lee_sent, stratify=joong_lee_sent, test_size=0.2, random_state=15) \n",
        "\n",
        "# 로지스틱에 적용\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "logistic = LogisticRegression()\n",
        "logistic.fit(x_train, y_train)\n",
        "\n",
        "print(logistic.score(x_train, y_train)) # estimation = 0.7372881355932204\n",
        "print(logistic.score(x_test, y_test)) # prediction = 0.7\n",
        "\n",
        "# 랜덤포레스트에 적용\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "forest = RandomForestClassifier(n_estimators = 100, n_jobs = -1, random_state=2018)\n",
        "forest = forest.fit(x_train, y_train)\n",
        " \n",
        "print(forest.score(x_train, y_train)) # estimation = 1.0\n",
        "print(forest.score(x_test, y_test)) # preditction = 0.6333333333333333\n",
        "\n",
        "# 랜덤포레스트 k-fold 각각의 ROC AUC 값\n",
        "k_fold = cross_val_score(forest, x_train, y_train, cv=10, scoring='roc_auc')\n",
        "print(k_fold)\n",
        "# [0.62962963 0.62962963 0.8125     0.875      0.78125    0.8125  0.25       0.859375   0.25       0.66666667]\n",
        "\n",
        "# k-fold 전체 ROC AUC 평균치\n",
        "score = np.mean(k_fold)\n",
        "print(score) # 0.6566550925925927\n",
        "\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "# 윤석열 독립변수 TF-IDF화\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf_vect = TfidfVectorizer()\n",
        "\n",
        "x_joong_yun = []\n",
        "for words in joong_yun_noun2:\n",
        "  words_join = ' '.join(words)\n",
        "  x_joong_yun.append(words_join)\n",
        "\n",
        "vect = tf_vect.fit(x_joong_yun)\n",
        "x_joong_yun = vect.transform(x_joong_yun)\n",
        "\n",
        "# training / test data 분류\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_joong_yun, joong_yun_sent, stratify=joong_yun_sent, test_size=0.2, random_state=15) \n",
        "\n",
        "# 로지스틱에 적용\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "logistic = LogisticRegression()\n",
        "logistic.fit(x_train, y_train)\n",
        "\n",
        "print(logistic.score(x_train, y_train)) # estimation = 0.811965811965812\n",
        "print(logistic.score(x_test, y_test)) # prediction = 0.7\n",
        "\n",
        "# 랜덤포레스트에 적용\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "forest = RandomForestClassifier(n_estimators = 100, n_jobs = -1, random_state=2018)\n",
        "forest = forest.fit(x_train, y_train)\n",
        " \n",
        "print(forest.score(x_train, y_train)) # estimation = 1.0\n",
        "print(forest.score(x_test, y_test)) # preditction = 0.7333333333333333\n",
        "\n",
        "# 랜덤포레스트 k-fold 각각의 ROC AUC 값\n",
        "k_fold = cross_val_score(forest, x_train, y_train, cv=10, scoring='roc_auc')\n",
        "print(k_fold)\n",
        "# [1.         0.65625    1.         0.59375    0.578125   0.765625  0.890625   0.5        0.97916667 0.77083333]\n",
        "\n",
        "# k-fold 전체 ROC AUC 평균치\n",
        "score = np.mean(k_fold)\n",
        "print(score) # 0.7734375"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKkNZLVRGAq9"
      },
      "source": [
        "ㄷ. 동아일보"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZLiaWiZGAjU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "## 감정단어 빈도 그래프, 한 번에 5분 정도 소요\n",
        "# 이재명\n",
        "emotion = []\n",
        "sent_list = []\n",
        "for sentences in donga_lee_noun2:\n",
        "  match_words = [sentence for sentence in sentences if sentence in list(NRC2[1])] # NRC2에 포함된 단어 \n",
        "  sentiment = 0\n",
        "  for word in match_words:\n",
        "    temp = list(NRC2.iloc[np.where(NRC2[1] == word)[0],1])[0] # 감정 리턴\n",
        "    score = list(NRC2.iloc[np.where(NRC2[1] == word)[0],2])[0] # 감정에 해당하는 수치 리턴\n",
        "    emotion.append(temp)\n",
        "    sentiment += int(score)\n",
        "  sent_list.append(sentiment)\n",
        "donga_lee_sent = pd.DataFrame(sent_list, columns = ['sentiment'])\n",
        "donga_lee_sent['sentiment'] = np.where(donga_lee_sent['sentiment']>=0, 'positive', 'negative')\n",
        "\n",
        "new_emotion = []\n",
        "for emo in emotion:\n",
        "  if emo == 'anger' or emo == 'disgust' or emo == 'fear' or emo == 'sadness' or emo == 'negative':\n",
        "    emo = 'negative'\n",
        "    new_emotion.append(emo)\n",
        "  else:\n",
        "    emo = 'positive'\n",
        "    new_emotion.append(emo)\n",
        "sentiment_result1 = pd.Series(emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정 그대로를 빈도로 표현한 것\n",
        "# anger(2253), trust(2014), anticipation(1511), fear(974), joy(790), disgust(242), sadness(135)\n",
        "\n",
        "sentiment_result1 = pd.Series(new_emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정을 positive와 negative로 분류해 표현한 것\n",
        "# positive(4315), negative(3604)\n",
        "# 결과 이미지는 3. 감성분석 폴더에 넣어놨습니다.\n",
        "\n",
        "\n",
        "# 윤석열\n",
        "emotion = []\n",
        "sent_list = []\n",
        "for sentences in donga_yun_noun2:\n",
        "  match_words = [sentence for sentence in sentences if sentence in list(NRC2[1])] # NRC2에 포함된 단어 \n",
        "  sentiment = 0\n",
        "  for word in match_words:\n",
        "    temp = list(NRC2.iloc[np.where(NRC2[1] == word)[0],1])[0] # 감정 리턴\n",
        "    score = list(NRC2.iloc[np.where(NRC2[1] == word)[0],2])[0] # 감정에 해당하는 수치 리턴\n",
        "    emotion.append(temp)\n",
        "    sentiment += int(score)\n",
        "  sent_list.append(sentiment)\n",
        "donga_yun_sent = pd.DataFrame(sent_list, columns = ['sentiment'])\n",
        "donga_yun_sent['sentiment'] = np.where(donga_yun_sent['sentiment']>=0, 'positive', 'negative')\n",
        "\n",
        "new_emotion = []\n",
        "for emo in emotion:\n",
        "  if emo == 'anger' or emo == 'disgust' or emo == 'fear' or emo == 'sadness' or emo == 'negative':\n",
        "    emo = 'negative'\n",
        "    new_emotion.append(emo)\n",
        "  else:\n",
        "    emo = 'positive'\n",
        "    new_emotion.append(emo)\n",
        "sentiment_result1 = pd.Series(emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정 그대로를 빈도로 표현한 것\n",
        "# anger(1831), trust(1700), anticipation(1263), fear(974), joy(713), disgust(236), sadness(124)\n",
        "\n",
        "sentiment_result1 = pd.Series(new_emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정을 positive와 negative로 분류해 표현한 것\n",
        "# positive(3676), negative(3165)\n",
        "# 결과 이미지는 3. 감성분석 폴더에 넣어놨습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUBDGdZaMZGQ"
      },
      "outputs": [],
      "source": [
        "## 감성분석\n",
        "# 이재명 독립변수 TF-IDF화\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf_vect = TfidfVectorizer()\n",
        "\n",
        "x_donga_lee = []\n",
        "for words in donga_lee_noun2:\n",
        "  words_join = ' '.join(words)\n",
        "  x_donga_lee.append(words_join)\n",
        "\n",
        "vect = tf_vect.fit(x_donga_lee)\n",
        "x_donga_lee = vect.transform(x_donga_lee)\n",
        "\n",
        "# training / test data 분류\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_donga_lee, donga_lee_sent, stratify=donga_lee_sent, test_size=0.2, random_state=15) \n",
        "\n",
        "# 로지스틱에 적용\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "logistic = LogisticRegression()\n",
        "logistic.fit(x_train, y_train)\n",
        "\n",
        "print(logistic.score(x_train, y_train)) # estimation = 0.78125\n",
        "print(logistic.score(x_test, y_test)) # prediction = 0.625\n",
        "\n",
        "# 랜덤포레스트에 적용\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "forest = RandomForestClassifier(n_estimators = 100, n_jobs = -1, random_state=2018)\n",
        "forest = forest.fit(x_train, y_train)\n",
        " \n",
        "print(forest.score(x_train, y_train)) # estimation = 1.0\n",
        "print(forest.score(x_test, y_test)) # preditction = 0.625\n",
        "\n",
        "# 랜덤포레스트 k-fold 각각의 ROC AUC 값\n",
        "k_fold = cross_val_score(forest, x_train, y_train, cv=10, scoring='roc_auc')\n",
        "print(k_fold)\n",
        "# [0.81318681 0.79120879 0.91666667 0.6025641  0.6474359  0.75  0.57738095 0.60119048 0.82142857 0.75595238]\n",
        "\n",
        "# k-fold 전체 ROC AUC 평균치\n",
        "score = np.mean(k_fold)\n",
        "print(score) # 0.7277014652014652\n",
        "\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "# 윤석열 독립변수 TF-IDF화\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf_vect = TfidfVectorizer()\n",
        "\n",
        "x_donga_yun = []\n",
        "for words in donga_yun_noun2:\n",
        "  words_join = ' '.join(words)\n",
        "  x_donga_yun.append(words_join)\n",
        "\n",
        "vect = tf_vect.fit(x_donga_yun)\n",
        "x_donga_yun = vect.transform(x_donga_yun)\n",
        "\n",
        "# training / test data 분류\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_donga_yun, donga_yun_sent, stratify=donga_yun_sent, test_size=0.2, random_state=15) \n",
        "\n",
        "# 로지스틱에 적용\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "logistic = LogisticRegression()\n",
        "logistic.fit(x_train, y_train)\n",
        "\n",
        "print(logistic.score(x_train, y_train)) # estimation = 0.8402366863905325\n",
        "print(logistic.score(x_test, y_test)) # prediction = 0.5581395348837209\n",
        "\n",
        "# 랜덤포레스트에 적용\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "forest = RandomForestClassifier(n_estimators = 100, n_jobs = -1, random_state=2018)\n",
        "forest = forest.fit(x_train, y_train)\n",
        " \n",
        "print(forest.score(x_train, y_train)) # estimation = 1.0\n",
        "print(forest.score(x_test, y_test)) # preditction = 0.627906976744186\n",
        "\n",
        "# 랜덤포레스트 k-fold 각각의 ROC AUC 값\n",
        "k_fold = cross_val_score(forest, x_train, y_train, cv=10, scoring='roc_auc')\n",
        "print(k_fold)\n",
        "# [0.75       0.73484848 0.84090909 0.78030303 0.64393939 0.81818182 0.83333333 0.78030303 0.72142857 0.88333333]\n",
        "\n",
        "# k-fold 전체 ROC AUC 평균치\n",
        "score = np.mean(k_fold)\n",
        "print(score) # 0.7786580086580087"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T4DjUjaGYEH"
      },
      "source": [
        "ㄹ. 경향신문"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHb6GsqsGW5Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "## 감정단어 빈도 그래프, 한 번에 5분 정도 소요\n",
        "# 이재명\n",
        "emotion = []\n",
        "sent_list = []\n",
        "for sentences in kyung_lee_noun2:\n",
        "  match_words = [sentence for sentence in sentences if sentence in list(NRC2[1])] # NRC2에 포함된 단어 \n",
        "  sentiment = 0\n",
        "  for word in match_words:\n",
        "    temp = list(NRC2.iloc[np.where(NRC2[1] == word)[0],1])[0] # 감정 리턴\n",
        "    score = list(NRC2.iloc[np.where(NRC2[1] == word)[0],2])[0] # 감정에 해당하는 수치 리턴\n",
        "    emotion.append(temp)\n",
        "    sentiment += int(score)\n",
        "  sent_list.append(sentiment)\n",
        "kyung_lee_sent = pd.DataFrame(sent_list, columns = ['sentiment'])\n",
        "kyung_lee_sent['sentiment'] = np.where(kyung_lee_sent['sentiment']>=0, 'positive', 'negative')\n",
        "\n",
        "new_emotion = []\n",
        "for emo in emotion:\n",
        "  if emo == 'anger' or emo == 'disgust' or emo == 'fear' or emo == 'sadness' or emo == 'negative':\n",
        "    emo = 'negative'\n",
        "    new_emotion.append(emo)\n",
        "  else:\n",
        "    emo = 'positive'\n",
        "    new_emotion.append(emo)\n",
        "sentiment_result1 = pd.Series(emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정 그대로를 빈도로 표현한 것\n",
        "# anger(2003), trust(1566), anticipation(1396), fear(885), joy(727), disgust(209), sadness(106)\n",
        "\n",
        "sentiment_result1 = pd.Series(new_emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정을 positive와 negative로 분류해 표현한 것\n",
        "# positive(3689), negative(3203)\n",
        "# 결과 이미지는 3. 감성분석 폴더에 넣어놨습니다.\n",
        "\n",
        "\n",
        "\n",
        "# 윤석열\n",
        "emotion = []\n",
        "sent_list = []\n",
        "for sentences in kyung_yun_noun2:\n",
        "  match_words = [sentence for sentence in sentences if sentence in list(NRC2[1])] # NRC2에 포함된 단어 \n",
        "  sentiment = 0\n",
        "  for word in match_words:\n",
        "    temp = list(NRC2.iloc[np.where(NRC2[1] == word)[0],1])[0] # 감정 리턴\n",
        "    score = list(NRC2.iloc[np.where(NRC2[1] == word)[0],2])[0] # 감정에 해당하는 수치 리턴\n",
        "    emotion.append(temp)\n",
        "    sentiment += int(score)\n",
        "  sent_list.append(sentiment)\n",
        "kyung_yun_sent = pd.DataFrame(sent_list, columns = ['sentiment'])\n",
        "kyung_yun_sent['sentiment'] = np.where(kyung_yun_sent['sentiment']>=0, 'positive', 'negative')\n",
        "\n",
        "new_emotion = []\n",
        "for emo in emotion:\n",
        "  if emo == 'anger' or emo == 'disgust' or emo == 'fear' or emo == 'sadness' or emo == 'negative':\n",
        "    emo = 'negative'\n",
        "    new_emotion.append(emo)\n",
        "  else:\n",
        "    emo = 'positive'\n",
        "    new_emotion.append(emo)\n",
        "sentiment_result1 = pd.Series(emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정 그대로를 빈도로 표현한 것\n",
        "# anger(2121), trust(1697), anticipation(1491), fear(1055), joy(830), disgust(261), sadness(134)\n",
        "\n",
        "sentiment_result1 = pd.Series(new_emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정을 positive와 negative로 분류해 표현한 것\n",
        "# positive(4018), negative(3571)\n",
        "# 결과 이미지는 3. 감성분석 폴더에 넣어놨습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpaUGhv6NKQN"
      },
      "outputs": [],
      "source": [
        "## 감성분석\n",
        "# 이재명 독립변수 TF-IDF화\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf_vect = TfidfVectorizer()\n",
        "\n",
        "x_kyung_lee = []\n",
        "for words in kyung_lee_noun2:\n",
        "  words_join = ' '.join(words)\n",
        "  x_kyung_lee.append(words_join)\n",
        "\n",
        "vect = tf_vect.fit(x_kyung_lee)\n",
        "x_kyung_lee = vect.transform(x_kyung_lee)\n",
        "\n",
        "# training / test data 분류\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_kyung_lee, kyung_lee_sent, stratify=kyung_lee_sent, test_size=0.2, random_state=15) \n",
        "\n",
        "# 로지스틱에 적용\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "logistic = LogisticRegression()\n",
        "logistic.fit(x_train, y_train)\n",
        "\n",
        "print(logistic.score(x_train, y_train)) # estimation = 0.785234899328859\n",
        "print(logistic.score(x_test, y_test)) # prediction = 0.6842105263157895\n",
        "\n",
        "# 랜덤포레스트에 적용\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "forest = RandomForestClassifier(n_estimators = 100, n_jobs = -1, random_state=2018)\n",
        "forest = forest.fit(x_train, y_train)\n",
        " \n",
        "print(forest.score(x_train, y_train)) # estimation = 1.0\n",
        "print(forest.score(x_test, y_test)) # preditction = 0.7105263157894737\n",
        "\n",
        "# 랜덤포레스트 k-fold 각각의 ROC AUC 값\n",
        "k_fold = cross_val_score(forest, x_train, y_train, cv=10, scoring='roc_auc')\n",
        "print(k_fold)\n",
        "# [0.54   0.89   0.58   0.76   0.43   0.76   0.66   0.55   0.7    0.8625]\n",
        "\n",
        "# k-fold 전체 ROC AUC 평균치\n",
        "score = np.mean(k_fold)\n",
        "print(score) # 0.67325\n",
        "\n",
        "#---------------------------------------------------------------\n",
        "\n",
        "# 윤석열 독립변수 TF-IDF화\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf_vect = TfidfVectorizer()\n",
        "\n",
        "x_kyung_yun = []\n",
        "for words in kyung_yun_noun2:\n",
        "  words_join = ' '.join(words)\n",
        "  x_kyung_yun.append(words_join)\n",
        "\n",
        "vect = tf_vect.fit(x_kyung_yun)\n",
        "x_kyung_yun = vect.transform(x_kyung_yun)\n",
        "\n",
        "# training / test data 분류\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_kyung_yun, kyung_yun_sent, stratify=kyung_yun_sent, test_size=0.2, random_state=15) \n",
        "\n",
        "# 로지스틱에 적용\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "logistic = LogisticRegression()\n",
        "logistic.fit(x_train, y_train)\n",
        "\n",
        "print(logistic.score(x_train, y_train)) # estimation = 0.8421052631578947\n",
        "print(logistic.score(x_test, y_test)) # prediction = 0.7674418604651163\n",
        "\n",
        "# 랜덤포레스트에 적용\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "forest = RandomForestClassifier(n_estimators = 100, n_jobs = -1, random_state=2018)\n",
        "forest = forest.fit(x_train, y_train)\n",
        " \n",
        "print(forest.score(x_train, y_train)) # estimation = 1.0\n",
        "print(forest.score(x_test, y_test)) # preditction = 0.7209302325581395\n",
        "\n",
        "# 랜덤포레스트 k-fold 각각의 ROC AUC 값\n",
        "k_fold = cross_val_score(forest, x_train, y_train, cv=10, scoring='roc_auc')\n",
        "print(k_fold)\n",
        "# [0.69480519 0.84848485 0.76515152 0.62121212 0.62878788 0.71212121 0.78787879 0.89393939 0.86363636 0.87121212]\n",
        "\n",
        "# k-fold 전체 ROC AUC 평균치\n",
        "score = np.mean(k_fold)\n",
        "print(score) # 0.7687229437229437"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Yl2Cs05GxHT"
      },
      "source": [
        "ㅁ. 한겨례"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_qhfPuIGw_-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "## 감정단어 빈도 그래프, 한 번에 5분 정도 소요\n",
        "# 이재명\n",
        "emotion = []\n",
        "sent_list = []\n",
        "for sentences in han_lee_noun2:\n",
        "  match_words = [sentence for sentence in sentences if sentence in list(NRC2[1])] # NRC2에 포함된 단어 \n",
        "  sentiment = 0\n",
        "  for word in match_words:\n",
        "    temp = list(NRC2.iloc[np.where(NRC2[1] == word)[0],1])[0] # 감정 리턴\n",
        "    score = list(NRC2.iloc[np.where(NRC2[1] == word)[0],2])[0] # 감정에 해당하는 수치 리턴\n",
        "    emotion.append(temp)\n",
        "    sentiment += int(score)\n",
        "  sent_list.append(sentiment)\n",
        "han_lee_sent = pd.DataFrame(sent_list, columns = ['sentiment'])\n",
        "han_lee_sent['sentiment'] = np.where(han_lee_sent['sentiment']>=0, 'positive', 'negative')\n",
        "\n",
        "new_emotion = []\n",
        "for emo in emotion:\n",
        "  if emo == 'anger' or emo == 'disgust' or emo == 'fear' or emo == 'sadness' or emo == 'negative':\n",
        "    emo = 'negative'\n",
        "    new_emotion.append(emo)\n",
        "  else:\n",
        "    emo = 'positive'\n",
        "    new_emotion.append(emo)\n",
        "sentiment_result1 = pd.Series(emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정 그대로를 빈도로 표현한 것\n",
        "# anger(1507), trust(1432), anticipation(1310), fear(661), joy(651), disgust(209), sadness(92)\n",
        "\n",
        "sentiment_result1 = pd.Series(new_emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정을 positive와 negative로 분류해 표현한 것\n",
        "# positive(3393), negative(2469)\n",
        "# 결과 이미지는 3. 감성분석 폴더에 넣어놨습니다.\n",
        "\n",
        "\n",
        "# 윤석열\n",
        "emotion = []\n",
        "sent_list = []\n",
        "for sentences in han_yun_noun2:\n",
        "  match_words = [sentence for sentence in sentences if sentence in list(NRC2[1])] # NRC2에 포함된 단어 \n",
        "  sentiment = 0\n",
        "  for word in match_words:\n",
        "    temp = list(NRC2.iloc[np.where(NRC2[1] == word)[0],1])[0] # 감정 리턴\n",
        "    score = list(NRC2.iloc[np.where(NRC2[1] == word)[0],2])[0] # 감정에 해당하는 수치 리턴\n",
        "    emotion.append(temp)\n",
        "    sentiment += int(score)\n",
        "  sent_list.append(sentiment)\n",
        "han_yun_sent = pd.DataFrame(sent_list, columns = ['sentiment'])\n",
        "han_yun_sent['sentiment'] = np.where(han_yun_sent['sentiment']>=0, 'positive', 'negative')\n",
        "\n",
        "new_emotion = []\n",
        "for emo in emotion:\n",
        "  if emo == 'anger' or emo == 'disgust' or emo == 'fear' or emo == 'sadness' or emo == 'negative':\n",
        "    emo = 'negative'\n",
        "    new_emotion.append(emo)\n",
        "  else:\n",
        "    emo = 'positive'\n",
        "    new_emotion.append(emo)\n",
        "sentiment_result1 = pd.Series(emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정 그대로를 빈도로 표현한 것\n",
        "# anger(1905), trust(1653), anticipation(1502), fear(958), joy(716), disgust(286), sadness(114)\n",
        "\n",
        "sentiment_result1 = pd.Series(new_emotion).value_counts()\n",
        "print(sentiment_result1, sentiment_result1.plot.bar())\n",
        "# 사용된 감정을 positive와 negative로 분류해 표현한 것\n",
        "# positive(3871), negative(3263)\n",
        "# 결과 이미지는 3. 감성분석 폴더에 넣어놨습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxvX5Ul0o9Cb"
      },
      "outputs": [],
      "source": [
        "## 감성분석\n",
        "# 이재명 독립변수 TF-IDF화\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf_vect = TfidfVectorizer()\n",
        "\n",
        "x_han_lee = []\n",
        "for words in han_lee_noun2:\n",
        "  words_join = ' '.join(words)\n",
        "  x_han_lee.append(words_join)\n",
        "\n",
        "vect = tf_vect.fit(x_han_lee)\n",
        "x_han_lee = vect.transform(x_han_lee)\n",
        "\n",
        "# training / test data 분류\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_han_lee, han_lee_sent, stratify=han_lee_sent, test_size=0.2, random_state=15) \n",
        "\n",
        "# 로지스틱에 적용\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "logistic = LogisticRegression()\n",
        "logistic.fit(x_train, y_train)\n",
        "\n",
        "print(logistic.score(x_train, y_train)) # estimation = 0.7391304347826086\n",
        "print(logistic.score(x_test, y_test)) # prediction = 0.7241379310344828\n",
        "\n",
        "# 랜덤포레스트에 적용\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "forest = RandomForestClassifier(n_estimators = 100, n_jobs = -1, random_state=2018)\n",
        "forest = forest.fit(x_train, y_train)\n",
        " \n",
        "print(forest.score(x_train, y_train)) # estimation = 1.0\n",
        "print(forest.score(x_test, y_test)) # preditction = 0.7241379310344828\n",
        "\n",
        "# 랜덤포레스트 k-fold 각각의 ROC AUC 값\n",
        "k_fold = cross_val_score(forest, x_train, y_train, cv=10, scoring='roc_auc')\n",
        "print(k_fold)\n",
        "#[0.46296296 0.74074074 0.53703704 0.59259259 0.2962963  0.52083333 0.29166667 0.60416667 0.625 0.79166667]\n",
        "\n",
        "# k-fold 전체 ROC AUC 평균치\n",
        "score = np.mean(k_fold)\n",
        "print(score) # 0.5462962962962963\n",
        "\n",
        "#---------------------------------------------------------------\n",
        "\n",
        "# 윤석열 독립변수 TF-IDF화\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf_vect = TfidfVectorizer()\n",
        "\n",
        "x_han_yun = []\n",
        "for words in han_yun_noun2:\n",
        "  words_join = ' '.join(words)\n",
        "  x_han_yun.append(words_join)\n",
        "\n",
        "vect = tf_vect.fit(x_han_yun)\n",
        "x_han_yun = vect.transform(x_han_yun)\n",
        "\n",
        "# training / test data 분류\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_han_yun, han_yun_sent, stratify=han_yun_sent, test_size=0.2, random_state=15) \n",
        "\n",
        "# 로지스틱에 적용\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "logistic = LogisticRegression()\n",
        "logistic.fit(x_train, y_train)\n",
        "\n",
        "print(logistic.score(x_train, y_train)) # estimation = 0.8014184397163121\n",
        "print(logistic.score(x_test, y_test)) # prediction = 0.6944444444444444\n",
        "\n",
        "# 랜덤포레스트에 적용\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "forest = RandomForestClassifier(n_estimators = 100, n_jobs = -1, random_state=2018)\n",
        "forest = forest.fit(x_train, y_train)\n",
        " \n",
        "print(forest.score(x_train, y_train)) # estimation = 1.0\n",
        "print(forest.score(x_test, y_test)) # preditction = 0.7222222222222222\n",
        "\n",
        "# 랜덤포레스트 k-fold 각각의 ROC AUC 값\n",
        "k_fold = cross_val_score(forest, x_train, y_train, cv=10, scoring='roc_auc')\n",
        "print(k_fold)\n",
        "# [0.7037037  1.         0.85555556 0.64444444 0.85555556 0.57777778 0.91111111 0.61111111 0.72222222 0.74444444]\n",
        "\n",
        "# k-fold 전체 ROC AUC 평균치\n",
        "score = np.mean(k_fold)\n",
        "print(score) # 0.7625925925925927"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 대체 이걸 어떻게 해석해야 하지?? 쉽지 않네\n",
        "# 1) 단어 표로 정리하신 것처럼 이것도 유사하게 정리하면 보기도 깔끔할 것 같고, 차이가 발생하는 곳이 있는지 분석도 용이할 것 같아요\n",
        "# 2) 언론사 전반적으로 예측률이 낮음.\n",
        "# 가장 높은 게 경향신문 윤석열 로지스틱 0.767,  가장 낮은 게 조선일보 윤석열 로지스틱/동아일보 이재명 로지스틱/동아일보 이재명 랜덤포레 0.625\n",
        "# 3) 로지스틱의 예측률이 랜덤포레스트보다 조금 더 높음\n",
        "\n",
        "# 예측률이 높으면 그만큼 해당 후보에 대해 반복되는 단어를 사용하는 것으로 판단할 수 있음.\n",
        "# 자주 사용되지 않는 단어를 사용했다면 높은 가중치를 가졌겠지만 예측에는 어렵게 작용\n",
        "# 여러 기사에 자주 반복되는 단어라면 낮은 가중치를 가졌더라도 지속적으로 종속변수에 영향을 미쳤을 것이므로 높은 예측률로 연결되지\n",
        "\n",
        "# 예를 들어서 경향일보의 윤석열 후보에 대한 로지스틱 모형 예측률은 0.767, 이재명 후보에 대한 로지스틱 모형의 예측률이 0.68\n",
        "# 높은 예측률은 윤석열 후보에 대한 기사를 다룸에 있어서 지속적으로 반복되는 단어를 사용했다고 판단 가능\n",
        "# 이재명 후보에 대해서는 자주 새로운 단어를 제시함으로써 기존 의혹으로부터 벗어나려는 의도가 다분할 것\n",
        "\n",
        "\n",
        "# 조선일보\n",
        "1. 이재명 로지스틱: 0.6949152542372882\n",
        "2. 이재명 랜덤포레: 0.6610169491525424\n",
        "3. 이재명 AUC 평균: 0.7243952546296295\n",
        "4. 윤석열 로지스틱: 0.625\n",
        "5. 윤석열 랜덤포레: 0.6875\n",
        "6. 윤석열 AUC 평균: 0.8010531135531135\n",
        "\n",
        "# 중앙일보\n",
        "1. 이재명 로지스틱: 0.7\n",
        "2. 이재명 랜덤포레: 0.6333333333333333\n",
        "3. 이재명 AUC 평균: 0.6566550925925927\n",
        "4. 윤석열 로지스틱: 0.7\n",
        "5. 윤석열 랜덤포레: 0.7333333333333333\n",
        "6. 윤석열 AUC 평균: 0.7734375\n",
        "\n",
        "# 동아일보\n",
        "1. 이재명 로지스틱: 0.625\n",
        "2. 이재명 랜덤포레: 0.625\n",
        "3. 이재명 AUC 평균: 0.7277014652014652\n",
        "4. 윤석열 로지스틱: 0.5581395348837209\n",
        "5. 윤석열 랜덤포레: 0.627906976744186\n",
        "6. 윤석열 AUC 평균: 0.7786580086580087\n",
        "\n",
        "# 경향신문\n",
        "1. 이재명 로지스틱: 0.6842105263157895\n",
        "2. 이재명 랜덤포레: 0.7105263157894737\n",
        "3. 이재명 AUC 평균: 0.67325\n",
        "4. 윤석열 로지스틱: 0.7674418604651163\n",
        "5. 윤석열 랜덤포레: 0.7209302325581395\n",
        "6. 윤석열 AUC 평균: 0.7687229437229437\n",
        "\n",
        "# 한겨례신문\n",
        "1. 이재명 로지스틱: 0.6944444444444444\n",
        "2. 이재명 랜덤포레: 0.7241379310344828\n",
        "3. 이재명 AUC 평균: 0.5462962962962963\n",
        "4. 윤석열 로지스틱: 0.6944444444444444\n",
        "5. 윤석열 랜덤포레: 0.7222222222222222\n",
        "6. 윤석열 AUC 평균: 0.7625925925925927"
      ],
      "metadata": {
        "id": "tdzvZrUwwQLg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "kH8bP7pUyZi7",
        "rsc2KAYGVGxy"
      ],
      "name": "2. 빈도 도출 및 3. 감성분석.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMUiycYVyF11ZEYgvIBFQE2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}