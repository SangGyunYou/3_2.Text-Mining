{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. 뉴스 추출.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SangGyunYou/Github_test/blob/main/1_%EB%89%B4%EC%8A%A4_%EC%B6%94%EC%B6%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "조선일보 이재명 292개 / 윤석열 240개\n",
        "-----------------------------------------\n",
        "중앙일보 이재명 148개 / 윤석열 147개\n",
        "-----------------------------------------\n",
        "동아일보 이재명 240개 / 윤석열 212개\n",
        "-----------------------------------------\n",
        "-> 보수 이재명 680개 / 윤석열 599개\n",
        "\n",
        "경향신문 이재명 187개 / 윤석열 214개\n",
        "-----------------------------------------\n",
        "한겨례 이재명 144개 / 윤석열 177개\n",
        "-----------------------------------------\n",
        "-> 진보 이재명 331개 / 윤석열 391개\n",
        "* 전반적으로 반대 진영 후보에 대한 기사를 더 많이 작성하는군요 호호"
      ],
      "metadata": {
        "id": "Epyljiw0sBjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ㄱ. 조선일보"
      ],
      "metadata": {
        "id": "B9tBYGyPWzo9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0p4ObDxlA65"
      },
      "outputs": [],
      "source": [
        "!pip install requests\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "#### 이재명: 411개\n",
        "url0 = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%EC%9D%B4%EC%9E%AC%EB%AA%85&sort=1&photo=3&field=0&pd=3&ds=2021.11.05&de=2021.12.20&mynews=1&office_type=1&office_section_code=1&news_office_checked=1023&nso=so:dd,p:from20211105to20211220,a:all&start='\n",
        "starts = list(range(1, 420, 10))\n",
        "\n",
        "url_base = []\n",
        "for start in starts:\n",
        "  url_base.append(url0 + str(start))\n",
        "soup_list = []\n",
        "for url in url_base:\n",
        "  resp = requests.get(url)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  soup_list.append(soup)\n",
        "\n",
        "# 네이버에서 제목, url 뽑기\n",
        "title_list = []\n",
        "url_list = []\n",
        "for soup in soup_list:\n",
        "  for i in range(0, 10):\n",
        "    try:\n",
        "      title = soup.select('a.news_tit')[i]['title']\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      news = soup.select('a.news_tit')[i]['href']\n",
        "    except:\n",
        "      continue\n",
        "    title_list.append(title)\n",
        "    url_list.append(news)\n",
        "\n",
        "# 네이버에서 뽑은 url에서 기사 추출, 5분 정도 소요\n",
        "detail_list = []\n",
        "for detail in url_list:\n",
        "  resp = requests.get(detail)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  article = soup.select('script#fusion-metadata')[0].text\n",
        "  m = re.search('={(.+?)date\"', article)\n",
        "  kor_word = re.sub(r'[^ㄱ-ㅎ|가-힣]+', ' ', m.group(1))\n",
        "  detail_list.append(kor_word)\n",
        "\n",
        "# 데이터 프레임 제작\n",
        "data_df = []\n",
        "for news in zip(title_list, url_list, detail_list):\n",
        "  df = {'title' : news[0],\n",
        "        'url' : news[1],\n",
        "        'detail' : news[2]\n",
        "        }\n",
        "  data_df.append(df)\n",
        "chosun_lee_df = pd.DataFrame(data_df)\n",
        "chosun_lee_df\n",
        "\n",
        "# 결과 csv로 저장, 좌측 코랩 파일에 나타나면 클릭해서 본인 노트북에 저장하시면 됩니다\n",
        "chosun_lee_df.to_csv('chosun_lee_df.csv', header=True, index=False)\n",
        "\n",
        "-----------------------------------------------------------\n",
        "\n",
        "#### 윤석열: 341개\n",
        "url0 = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%EC%9C%A4%EC%84%9D%EC%97%B4&sort=1&photo=3&field=0&pd=3&ds=2021.11.05&de=2021.12.20&mynews=1&office_type=1&office_section_code=1&news_office_checked=1023&nso=so:dd,p:from20211105to20211220,a:all&start='\n",
        "starts = list(range(1, 350, 10))\n",
        "\n",
        "url_base = []\n",
        "for start in starts:\n",
        "  url_base.append(url0 + str(start))\n",
        "soup_list = []\n",
        "for url in url_base:\n",
        "  resp = requests.get(url)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  soup_list.append(soup)\n",
        "\n",
        "# 네이버에서 제목, url 뽑기\n",
        "title_list = []\n",
        "url_list = []\n",
        "for soup in soup_list:\n",
        "  for i in range(0, 10):\n",
        "    try:\n",
        "      title = soup.select('a.news_tit')[i]['title']\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      news = soup.select('a.news_tit')[i]['href']\n",
        "    except:\n",
        "      continue\n",
        "    title_list.append(title)\n",
        "    url_list.append(news)\n",
        "\n",
        "# 네이버에서 뽑은 url에서 기사 추출, 5분 정도 소요\n",
        "detail_list = []\n",
        "for detail in url_list:\n",
        "  resp = requests.get(detail)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  article = soup.select('script#fusion-metadata')[0].text\n",
        "  m = re.search('={(.+?)date\"', article)\n",
        "  kor_word = re.sub(r'[^ㄱ-ㅎ|가-힣]+', ' ', m.group(1))\n",
        "  detail_list.append(kor_word)\n",
        "\n",
        "# 데이터 프레임 제작\n",
        "data_df = []\n",
        "for news in zip(title_list, url_list, detail_list):\n",
        "  df = {'title' : news[0],\n",
        "        'url' : news[1],\n",
        "        'detail' : news[2]\n",
        "        }\n",
        "  data_df.append(df)\n",
        "chosun_yun_df = pd.DataFrame(data_df)\n",
        "chosun_yun_df\n",
        "\n",
        "# 결과 csv로 저장, 좌측 코랩 파일에 나타나면 클릭해서 본인 노트북에 저장하시면 됩니다\n",
        "chosun_yun_df.to_csv('chosun_yun_df.csv', header=True, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ㄴ. 중앙일보"
      ],
      "metadata": {
        "id": "yXwL07kNXHs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### 기간 늘어나면 url0와 starts 수정 필요\n",
        "!pip install requests\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "#### 이재명: 231개\n",
        "url0 = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%EC%9D%B4%EC%9E%AC%EB%AA%85&sort=1&photo=3&field=0&pd=3&ds=2021.11.05&de=2021.12.20&mynews=1&office_type=1&office_section_code=1&news_office_checked=1025&nso=so:dd,p:from20211105to20211220,a:all&start='\n",
        "starts = list(range(1, 240, 10))\n",
        "\n",
        "url_base = []\n",
        "for start in starts:\n",
        "  url_base.append(url0 + str(start))\n",
        "soup_list = []\n",
        "for url in url_base:\n",
        "  resp = requests.get(url)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  soup_list.append(soup)\n",
        "\n",
        "# 네이버에서 제목, url 뽑기\n",
        "title_list = []\n",
        "url_list = []\n",
        "for soup in soup_list:\n",
        "  for i in range(0, 10):\n",
        "    try:\n",
        "      title = soup.select('a.news_tit')[i]['title']\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      news = soup.select('a.news_tit')[i]['href']\n",
        "    except:\n",
        "      continue\n",
        "    title_list.append(title)\n",
        "    url_list.append(news)\n",
        "\n",
        "# 네이버에서 뽑은 url에서 기사 추출, 5분 정도 소요\n",
        "detail_list = []\n",
        "for detail in url_list:\n",
        "  resp = requests.get(detail)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  kor_word = ''\n",
        "  for p in soup.select('div#article_body > p'):\n",
        "    kor_word += p.text\n",
        "  kor_word = re.sub(r'\\xa0', ' ', kor_word)\n",
        "  detail_list.append(kor_word)\n",
        "\n",
        "# 데이터 프레임 제작\n",
        "data_df = []\n",
        "for news in zip(title_list, url_list, detail_list):\n",
        "  df = {'title' : news[0],\n",
        "        'url' : news[1],\n",
        "        'detail' : news[2]\n",
        "        }\n",
        "  data_df.append(df)\n",
        "joongang_lee_df = pd.DataFrame(data_df)\n",
        "joongang_lee_df\n",
        "\n",
        "# 결과 csv로 저장, 좌측 코랩 파일에 나타나면 클릭해서 본인 노트북에 저장하시면 됩니다\n",
        "joongang_lee_df.to_csv('joongang_lee_df.csv', header=True, index=False)\n",
        "\n",
        "-----------------------------------------------------------\n",
        "\n",
        "#### 윤석열: 221개\n",
        "url0 = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%EC%9C%A4%EC%84%9D%EC%97%B4&sort=1&photo=3&field=0&pd=3&ds=2021.11.05&de=2021.12.20&mynews=1&office_type=1&office_section_code=1&news_office_checked=1025&nso=so:dd,p:from20211105to20211220,a:all&start=https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%EC%9C%A4%EC%84%9D%EC%97%B4&sort=1&photo=3&field=0&pd=3&ds=2021.11.05&de=2021.12.20&mynews=1&office_type=1&office_section_code=1&news_office_checked=1025&nso=so:dd,p:from20211105to20211220,a:all&start='\n",
        "starts = list(range(1, 230, 10))\n",
        "\n",
        "url_base = []\n",
        "for start in starts:\n",
        "  url_base.append(url0 + str(start))\n",
        "soup_list = []\n",
        "for url in url_base:\n",
        "  resp = requests.get(url)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  soup_list.append(soup)\n",
        "\n",
        "# 네이버에서 제목, url 뽑기\n",
        "title_list = []\n",
        "url_list = []\n",
        "for soup in soup_list:\n",
        "  for i in range(0, 10):\n",
        "    try:\n",
        "      title = soup.select('a.news_tit')[i]['title']\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      news = soup.select('a.news_tit')[i]['href']\n",
        "    except:\n",
        "      continue\n",
        "    title_list.append(title)\n",
        "    url_list.append(news)\n",
        "\n",
        "# 네이버에서 뽑은 url에서 기사 추출, 5분 정도 소요\n",
        "detail_list = []\n",
        "for detail in url_list:\n",
        "  resp = requests.get(detail)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  kor_word = ''\n",
        "  for p in soup.select('div#article_body > p'):\n",
        "    kor_word += p.text\n",
        "  kor_word = re.sub(r'\\xa0', ' ', kor_word)\n",
        "  detail_list.append(kor_word)\n",
        "\n",
        "# 데이터 프레임 제작\n",
        "data_df = []\n",
        "for news in zip(title_list, url_list, detail_list):\n",
        "  df = {'title' : news[0],\n",
        "        'url' : news[1],\n",
        "        'detail' : news[2]\n",
        "        }\n",
        "  data_df.append(df)\n",
        "joongang_yun_df = pd.DataFrame(data_df)\n",
        "joongang_yun_df\n",
        "\n",
        "# 결과 csv로 저장, 좌측 코랩 파일에 나타나면 클릭해서 본인 노트북에 저장하시면 됩니다\n",
        "joongang_yun_df.to_csv('joongang_yun_df.csv', header=True, index=False)"
      ],
      "metadata": {
        "id": "-NRr_iyFj8jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ㄷ. 동아일보"
      ],
      "metadata": {
        "id": "7is6y7YVW9R3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### 기간 늘어나면 url0와 starts 수정 필요\n",
        "!pip install requests\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "#### 이재명: 341개\n",
        "url0 = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%EC%9D%B4%EC%9E%AC%EB%AA%85&sort=1&photo=3&field=0&pd=3&ds=2021.11.05&de=2021.12.20&mynews=1&office_type=1&office_section_code=1&news_office_checked=1020&nso=so:dd,p:from20211105to20211220,a:all&start='\n",
        "starts = list(range(1, 350, 10))\n",
        "\n",
        "url_base = []\n",
        "for start in starts:\n",
        "  url_base.append(url0 + str(start))\n",
        "soup_list = []\n",
        "for url in url_base:\n",
        "  resp = requests.get(url)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  soup_list.append(soup)\n",
        "\n",
        "# 네이버에서 제목, url 뽑기\n",
        "title_list = []\n",
        "url_list = []\n",
        "for soup in soup_list:\n",
        "  for i in range(0, 10):\n",
        "    try:\n",
        "      title = soup.select('a.news_tit')[i]['title']\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      news = soup.select('a.news_tit')[i]['href']\n",
        "    except:\n",
        "      continue\n",
        "    title_list.append(title)\n",
        "    url_list.append(news)\n",
        "\n",
        "# 네이버에서 뽑은 url에서 기사 추출, 5분 정도 소요\n",
        "detail_list = []\n",
        "for detail in url_list:\n",
        "  resp = requests.get(detail)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  article = soup.select('div.article_txt')[0].text.strip()\n",
        "  article = re.sub(r'\\r', ' ', article)\n",
        "  article = re.sub(r'\\n', ' ', article)\n",
        "  article = re.search('(.+?)창닫기기사를', article)\n",
        "  detail_list.append(article.group(1))\n",
        "\n",
        "# 데이터 프레임 제작\n",
        "data_df = []\n",
        "for news in zip(title_list, url_list, detail_list):\n",
        "  df = {'title' : news[0],\n",
        "        'url' : news[1],\n",
        "        'detail' : news[2]\n",
        "        }\n",
        "  data_df.append(df)\n",
        "donga_lee_df = pd.DataFrame(data_df)\n",
        "donga_lee_df\n",
        "\n",
        "# 결과 csv로 저장, 좌측 코랩 파일에 나타나면 클릭해서 본인 노트북에 저장하시면 됩니다\n",
        "donga_lee_df.to_csv('donga_lee_df.csv', header=True, index=False)\n",
        "\n",
        "-----------------------------------------------------------\n",
        "\n",
        "#### 윤석열: 311개\n",
        "url0 = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%EC%9C%A4%EC%84%9D%EC%97%B4&sort=1&photo=3&field=0&pd=3&ds=2021.11.05&de=2021.12.20&mynews=1&office_type=1&office_section_code=1&news_office_checked=1020&nso=so:dd,p:from20211105to20211220,a:all&start='\n",
        "starts = list(range(1, 320, 10))\n",
        "\n",
        "url_base = []\n",
        "for start in starts:\n",
        "  url_base.append(url0 + str(start))\n",
        "soup_list = []\n",
        "for url in url_base:\n",
        "  resp = requests.get(url)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  soup_list.append(soup)\n",
        "\n",
        "# 네이버에서 제목, url 뽑기\n",
        "title_list = []\n",
        "url_list = []\n",
        "for soup in soup_list:\n",
        "  for i in range(0, 10):\n",
        "    try:\n",
        "      title = soup.select('a.news_tit')[i]['title']\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      news = soup.select('a.news_tit')[i]['href']\n",
        "    except:\n",
        "      continue\n",
        "    title_list.append(title)\n",
        "    url_list.append(news)\n",
        "\n",
        "# 네이버에서 뽑은 url에서 기사 추출, 5분 정도 소요\n",
        "detail_list = []\n",
        "for detail in url_list:\n",
        "  resp = requests.get(detail)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  article = soup.select('div.article_txt')[0].text.strip()\n",
        "  article = re.sub(r'\\r', ' ', article)\n",
        "  article = re.sub(r'\\n', ' ', article)\n",
        "  article = re.search('(.+?)창닫기기사를', article)\n",
        "  detail_list.append(article.group(1))\n",
        "\n",
        "# 데이터 프레임 제작\n",
        "data_df = []\n",
        "for news in zip(title_list, url_list, detail_list):\n",
        "  df = {'title' : news[0],\n",
        "        'url' : news[1],\n",
        "        'detail' : news[2]\n",
        "        }\n",
        "  data_df.append(df)\n",
        "donga_yun_df = pd.DataFrame(data_df)\n",
        "donga_yun_df\n",
        "\n",
        "# 결과 csv로 저장, 좌측 코랩 파일에 나타나면 클릭해서 본인 노트북에 저장하시면 됩니다\n",
        "donga_yun_df.to_csv('donga_yun_df.csv', header=True, index=False)"
      ],
      "metadata": {
        "id": "AU_x-b27zN-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ㄹ. 경향신문"
      ],
      "metadata": {
        "id": "U2z31KP0W-_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### 기간 늘어나면 url0와 starts 수정 필요\n",
        "!pip install requests\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "#### 이재명: 271개\n",
        "url0 = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%EC%9D%B4%EC%9E%AC%EB%AA%85&sort=1&photo=3&field=0&pd=3&ds=2021.11.05&de=2021.12.20&mynews=1&office_type=1&office_section_code=1&news_office_checked=1032&nso=so:dd,p:from20211105to20211220,a:all&start='\n",
        "starts = list(range(1, 280, 10))\n",
        "\n",
        "url_base = []\n",
        "for start in starts:\n",
        "  url_base.append(url0 + str(start))\n",
        "soup_list = []\n",
        "for url in url_base:\n",
        "  resp = requests.get(url)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  soup_list.append(soup)\n",
        "\n",
        "# 네이버에서 제목, url 뽑기\n",
        "title_list = []\n",
        "url_list = []\n",
        "for soup in soup_list:\n",
        "  for i in range(0, 10):\n",
        "    try:\n",
        "      title = soup.select('a.news_tit')[i]['title']\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      news = soup.select('a.news_tit')[i]['href']\n",
        "    except:\n",
        "      continue\n",
        "    title_list.append(title)\n",
        "    url_list.append(news)\n",
        "\n",
        "# 네이버에서 뽑은 url에서 기사 추출, 5분 정도 소요\n",
        "detail_list = []\n",
        "headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.76 Whale/3.12.129.34 Safari/537.36'}\n",
        "for detail in url_list:\n",
        "  resp = requests.get(detail, headers=headers)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  article = ''\n",
        "  for p in soup.select('div.art_body > p.content_text'):\n",
        "    content = p.text.strip()\n",
        "    article += content\n",
        "  detail_list.append(article)\n",
        "\n",
        "# 데이터 프레임 제작\n",
        "data_df = []\n",
        "for news in zip(title_list, url_list, detail_list):\n",
        "  df = {'title' : news[0],\n",
        "        'url' : news[1],\n",
        "        'detail' : news[2]\n",
        "        }\n",
        "  data_df.append(df)\n",
        "kyung_lee_df = pd.DataFrame(data_df)\n",
        "kyung_lee_df\n",
        "\n",
        "# 174번 기사(https://www.khan.co.kr/opinion/notice/article/202111072054015)의 데이터가 비어있는데,\n",
        "# 혼자서만 유일하게 다른 태그를 쓰고 있네요. 이건 그냥 아래 내용으로 채우면 될 것 같습니당\n",
        "text = '경향신문 11월5일자 <남욱 “2010년 A씨에게 3000만원 줬고, 그 사람이 이재명에 로비” 2015년 수원지검서 진술> 기사와 관련해 이재명 더불어민주당 대선 후보 측은 “이재명 당시 성남시장은 남욱을 전혀 알지 못하고, A씨가 남욱으로부터 돈을 받았는지 여부에 관해서도 전혀 알지 못하며, A씨로부터 민간개발 방식이든 무엇이든 인허가 로비를 받은 적이 전혀 없음은 물론이고 A씨를 접촉한 적도 없다”면서 “이 후보는 성남시장 재직 시절 대장동 개발사업을 공영개발로 일관되게 추진했다”고 알려왔습니다.'\n",
        "kyung_lee_df['detail'][173] = text\n",
        "\n",
        "# 결과 csv로 저장, 좌측 코랩 파일에 나타나면 클릭해서 본인 노트북에 저장하시면 됩니다\n",
        "kyung_lee_df.to_csv('kyung_lee_df.csv', header=True, index=False)\n",
        "\n",
        "-----------------------------------------------------------\n",
        "\n",
        "#### 윤석열: 311개\n",
        "url0 = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%EC%9C%A4%EC%84%9D%EC%97%B4&sort=1&photo=3&field=0&pd=3&ds=2021.11.05&de=2021.12.20&mynews=1&office_type=1&office_section_code=1&news_office_checked=1032&nso=so:dd,p:from20211105to20211220,a:all&start='\n",
        "starts = list(range(1, 320, 10))\n",
        "\n",
        "url_base = []\n",
        "for start in starts:\n",
        "  url_base.append(url0 + str(start))\n",
        "soup_list = []\n",
        "for url in url_base:\n",
        "  resp = requests.get(url)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  soup_list.append(soup)\n",
        "\n",
        "# 네이버에서 제목, url 뽑기\n",
        "title_list = []\n",
        "url_list = []\n",
        "for soup in soup_list:\n",
        "  for i in range(0, 10):\n",
        "    try:\n",
        "      title = soup.select('a.news_tit')[i]['title']\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      news = soup.select('a.news_tit')[i]['href']\n",
        "    except:\n",
        "      continue\n",
        "    title_list.append(title)\n",
        "    url_list.append(news)\n",
        "\n",
        "# 네이버에서 뽑은 url에서 기사 추출, 5분 정도 소요\n",
        "detail_list = []\n",
        "headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.76 Whale/3.12.129.34 Safari/537.36'}\n",
        "for detail in url_list:\n",
        "  resp = requests.get(detail, headers=headers)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  article = ''\n",
        "  for p in soup.select('div.art_body > p.content_text'):\n",
        "    content = p.text.strip()\n",
        "    article += content\n",
        "  detail_list.append(article)\n",
        "\n",
        "# 데이터 프레임 제작\n",
        "data_df = []\n",
        "for news in zip(title_list, url_list, detail_list):\n",
        "  df = {'title' : news[0],\n",
        "        'url' : news[1],\n",
        "        'detail' : news[2]\n",
        "        }\n",
        "  data_df.append(df)\n",
        "kyung_yun_df = pd.DataFrame(data_df)\n",
        "kyung_yun_df\n",
        "\n",
        "# 결과 csv로 저장, 좌측 코랩 파일에 나타나면 클릭해서 본인 노트북에 저장하시면 됩니다\n",
        "kyung_yun_df.to_csv('kyung_yun_df.csv', header=True, index=False)"
      ],
      "metadata": {
        "id": "r07eA1LH8elf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ㅁ. 한겨례"
      ],
      "metadata": {
        "id": "U3YA47bgX-rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### 기간 늘어나면 url0와 starts 수정 필요\n",
        "!pip install requests\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "#### 이재명: 221개\n",
        "url0 = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%EC%9D%B4%EC%9E%AC%EB%AA%85&sort=1&photo=3&field=0&pd=3&ds=2021.11.05&de=2021.12.20&mynews=1&office_type=1&office_section_code=1&news_office_checked=1028&nso=so:dd,p:from20211105to20211220,a:all&start='\n",
        "starts = list(range(1, 230, 10))\n",
        "\n",
        "url_base = []\n",
        "for start in starts:\n",
        "  url_base.append(url0 + str(start))\n",
        "soup_list = []\n",
        "for url in url_base:\n",
        "  resp = requests.get(url)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  soup_list.append(soup)\n",
        "\n",
        "# 네이버에서 제목, url 뽑기\n",
        "title_list = []\n",
        "url_list = []\n",
        "for soup in soup_list:\n",
        "  for i in range(0, 10):\n",
        "    try:\n",
        "      title = soup.select('a.news_tit')[i]['title']\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      news = soup.select('a.news_tit')[i]['href']\n",
        "    except:\n",
        "      continue\n",
        "    title_list.append(title)\n",
        "    url_list.append(news)\n",
        "\n",
        "# 네이버에서 뽑은 url에서 기사 추출, 5분 정도 소요\n",
        "detail_list = []\n",
        "for detail in url_list:\n",
        "  resp = requests.get(detail)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  article = soup.select('div.text')[0].text\n",
        "  article = re.sub(r'\\n', ' ', article)\n",
        "  article = re.sub(r'\\r', ' ', article)\n",
        "  detail_list.append(article)\n",
        "\n",
        "# 데이터 프레임 제작\n",
        "data_df = []\n",
        "for news in zip(title_list, url_list, detail_list):\n",
        "  df = {'title' : news[0],\n",
        "        'url' : news[1],\n",
        "        'detail' : news[2]\n",
        "        }\n",
        "  data_df.append(df)\n",
        "han_lee_df = pd.DataFrame(data_df)\n",
        "han_lee_df\n",
        "\n",
        "# 결과 csv로 저장, 좌측 코랩 파일에 나타나면 클릭해서 본인 노트북에 저장하시면 됩니다\n",
        "han_lee_df.to_csv('han_lee_df.csv', header=True, index=False)\n",
        "\n",
        "-----------------------------------------------------------\n",
        "\n",
        "#### 윤석열: 271개\n",
        "url0 = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%EC%9C%A4%EC%84%9D%EC%97%B4&sort=1&photo=3&field=0&pd=3&ds=2021.11.05&de=2021.12.20&mynews=1&office_type=1&office_section_code=1&news_office_checked=1028&nso=so:dd,p:from20211105to20211220,a:all&start='\n",
        "starts = list(range(1, 280, 10))\n",
        "\n",
        "url_base = []\n",
        "for start in starts:\n",
        "  url_base.append(url0 + str(start))\n",
        "soup_list = []\n",
        "for url in url_base:\n",
        "  resp = requests.get(url)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  soup_list.append(soup)\n",
        "\n",
        "# 네이버에서 제목, url 뽑기\n",
        "title_list = []\n",
        "url_list = []\n",
        "for soup in soup_list:\n",
        "  for i in range(0, 10):\n",
        "    try:\n",
        "      title = soup.select('a.news_tit')[i]['title']\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      news = soup.select('a.news_tit')[i]['href']\n",
        "    except:\n",
        "      continue\n",
        "    title_list.append(title)\n",
        "    url_list.append(news)\n",
        "\n",
        "# 네이버에서 뽑은 url에서 기사 추출, 5분 정도 소요\n",
        "detail_list = []\n",
        "for detail in url_list:\n",
        "  resp = requests.get(detail)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "  article = soup.select('div.text')[0].text\n",
        "  article = re.sub(r'\\n', ' ', article)\n",
        "  article = re.sub(r'\\r', ' ', article)\n",
        "  detail_list.append(article)\n",
        "\n",
        "# 데이터 프레임 제작\n",
        "data_df = []\n",
        "for news in zip(title_list, url_list, detail_list):\n",
        "  df = {'title' : news[0],\n",
        "        'url' : news[1],\n",
        "        'detail' : news[2]\n",
        "        }\n",
        "  data_df.append(df)\n",
        "han_yun_df = pd.DataFrame(data_df)\n",
        "han_yun_df\n",
        "\n",
        "# 결과 csv로 저장, 좌측 코랩 파일에 나타나면 클릭해서 본인 노트북에 저장하시면 됩니다\n",
        "han_yun_df.to_csv('han_yun_df.csv', header=True, index=False)\n"
      ],
      "metadata": {
        "id": "RHQ22A2mV_yi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}